{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "# from unet import AttUNet, UNet, UNetWithPretrainedEncoder\n",
    "# from dataloader import ImageDataset, TransformWrapper\n",
    "import os\n",
    "import random\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torchvision import transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from PIL.ImageFile import ImageFile\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "def apply_clahe(pil_img):\n",
    "    # Convert PIL image to NumPy array\n",
    "    img = np.array(pil_img)\n",
    "\n",
    "    # If grayscale\n",
    "    if len(img.shape) == 2:\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        img_clahe = clahe.apply(img)\n",
    "    # If RGB\n",
    "    elif len(img.shape) == 3:\n",
    "        img_lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "        l, a, b = cv2.split(img_lab)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        l_clahe = clahe.apply(l)\n",
    "        img_lab_clahe = cv2.merge((l_clahe, a, b))\n",
    "        img_clahe = cv2.cvtColor(img_lab_clahe, cv2.COLOR_LAB2RGB)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported image format\")\n",
    "\n",
    "    # Convert back to PIL image\n",
    "    return Image.fromarray(img_clahe)\n",
    "\n",
    "\n",
    "class TransformWrapper:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.image_transforms = T.Compose([\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.RandomVerticalFlip(p=0.5),\n",
    "            T.RandomRotation(degrees=[30, 60, 90, 120, 150]),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        self.mask_transforms = T.Compose([\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.RandomVerticalFlip(p=0.5),\n",
    "            T.RandomRotation(degrees=45),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        # Apply the same random seed to ensure consistent transformations\n",
    "        seed = torch.randint(0, 2**32, (1, )).item()\n",
    "        torch.manual_seed(seed)\n",
    "        image = self.image_transforms(image)\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "        mask = self.mask_transforms(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "class ImageDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 images: list[ImageFile],\n",
    "                 masks: list[ImageFile],\n",
    "                 transform: bool = False,\n",
    "                 image_size: int = 224):\n",
    "\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self,\n",
    "                    index) -> tuple[torch.FloatTensor, torch.FloatTensor]:\n",
    "        image = self.images[index]\n",
    "        mask = self.masks[index]\n",
    "\n",
    "        image = image.resize((self.image_size, self.image_size),\n",
    "                             Image.Resampling.LANCZOS)\n",
    "        mask = mask.resize((self.image_size, self.image_size),\n",
    "                           Image.Resampling.LANCZOS)\n",
    "\n",
    "        image = apply_clahe(image)\n",
    "        image = T.GaussianBlur(3)(image)\n",
    "\n",
    "        mask = mask.convert(\"L\")  # Ensure mask is in grayscale\n",
    "        binary_threshold = 100  # Adjust this threshold as needed\n",
    "        mask = mask.point(lambda p: 255 if p > binary_threshold else 0)\n",
    "        mask = mask.convert('1')\n",
    "\n",
    "        normalize_tensor = T.Compose([\n",
    "            T.Lambda(lambda x: x.convert(\"RGB\")),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            T.Lambda(lambda x: x),\n",
    "        ])\n",
    "\n",
    "        if self.transform:\n",
    "            angle = random.uniform(-90, 90)\n",
    "\n",
    "            image = F.rotate(image, angle)\n",
    "            image = normalize_tensor(image)\n",
    "            mask = T.ToTensor()(F.rotate(mask, angle))\n",
    "            return image, mask\n",
    "\n",
    "        return normalize_tensor(image), T.ToTensor()(mask)\n",
    "\n",
    "\n",
    "data_pth = Path(\n",
    "    '/Users/tsakalis/Downloads/ECImageAnalysisMouse/New Binary Masks')\n",
    "blastocyst_pth = Path('/home/tsakalis/ntua/phd/cellforge/cellforge/data')\n",
    "\n",
    "full_path = \"/Users/tsakalis/ntua/cellforge/data/D2016.07.08_S1366_I149_11\"\n",
    "\n",
    "blastocyst_images_pth = blastocyst_pth / 'annotation_pn/images_pn'\n",
    "blastocyst_msk_pth = blastocyst_pth / 'annotation_pn/masks_pn'\n",
    "\n",
    "smooth = 1e-15\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "\n",
    "def dice_coef(y_pred, y_true):\n",
    "\n",
    "    intersection = torch.sum(y_true.flatten() * y_pred.flatten())\n",
    "    return (2. * intersection + smooth) / (\n",
    "        torch.sum(y_true).flatten() + torch.sum(y_pred).flatten() + smooth)\n",
    "\n",
    "\n",
    "def dice_loss(y_pred, y_true):\n",
    "\n",
    "    return 1.0 - dice_coef(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_file_paths = sorted(list((data_pth / \"images\").glob('*.png')),\n",
    "#                           key=lambda x: x.stem)\n",
    "# mask_file_paths = sorted(list((data_pth / \"masks\").glob('*.png')),\n",
    "#                          key=lambda x: x.stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.ndimage import center_of_mass\n",
    "\n",
    "\n",
    "def crop_around_center(image: Image.Image, mask: Image.Image,\n",
    "                       crop_size: int) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Finds the center of mass of the non-zero pixels in the image\n",
    "    and crops the image around that point.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image.Image): Input image (grayscale or binary recommended).\n",
    "        crop_size (int): Size of the square crop (e.g., 128 for 128x128 crop).\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image.Image: Cropped image around the center of mass.\n",
    "    \"\"\"\n",
    "    # Convert image to grayscale and NumPy array\n",
    "    image_array = np.array(image.convert(\"L\"))\n",
    "\n",
    "    # Find the center of mass of non-zero pixels\n",
    "    com = center_of_mass(image_array)\n",
    "\n",
    "    # Round to integers for pixel indexing\n",
    "    center_y, center_x = map(int, com)\n",
    "\n",
    "    # Calculate crop box\n",
    "    half_crop = crop_size // 2\n",
    "    left = max(center_x - half_crop, 0)\n",
    "    upper = max(center_y - half_crop, 0)\n",
    "    right = min(center_x + half_crop, image.width)\n",
    "    lower = min(center_y + half_crop, image.height)\n",
    "\n",
    "    # Crop the image\n",
    "    cropped_image = image.crop((left, upper, right, lower))\n",
    "\n",
    "    cropped_mask = mask.crop((left, upper, right, lower))\n",
    "    return cropped_image, cropped_mask\n",
    "\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file_paths = sorted(list(blastocyst_images_pth.glob('*.jpg')),\n",
    "                          key=lambda x: x.stem)\n",
    "mask_file_paths = sorted(list(blastocyst_msk_pth.glob('*.png')),\n",
    "                         key=lambda x: x.stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/tsakalis/ntua/phd/cellforge/cellforge/data/annotation_pn/images_pn')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blastocyst_images_pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m     13\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(c)\n\u001b[0;32m---> 15\u001b[0m images, masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mc)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# def remove_alpha(img):\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#     if img.mode == 'RGBA':  # If image has an alpha channel\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#         background = Image.new('RGB', img.size,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#     cropped_images.append(cropped)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#     cropped_masks.append(cropped_mask)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m ImageDataset(images\u001b[38;5;241m=\u001b[39mimages[:\u001b[38;5;241m400\u001b[39m],\n\u001b[1;32m     39\u001b[0m                              masks\u001b[38;5;241m=\u001b[39mmasks[:\u001b[38;5;241m400\u001b[39m],\n\u001b[1;32m     40\u001b[0m                              transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "# image_file_paths = sorted(list((data_pth / \"images\").glob('*.jpg')),\n",
    "#                           key=lambda x: x.stem)\n",
    "# mask_file_paths = sorted(list((data_pth / \"masks\").glob('*.png')),\n",
    "#                          key=lambda x: x.stem)\n",
    "\n",
    "# print(\"... Loading images ...\")\n",
    "images = [Image.open(img_path) for img_path in tqdm(image_file_paths)]\n",
    "masks = [Image.open(msk_pth) for msk_pth in tqdm(mask_file_paths)]\n",
    "\n",
    "c = list(zip(images, masks))\n",
    "import random\n",
    "\n",
    "random.shuffle(c)\n",
    "\n",
    "images, masks = zip(*c)\n",
    "\n",
    "# def remove_alpha(img):\n",
    "#     if img.mode == 'RGBA':  # If image has an alpha channel\n",
    "#         background = Image.new('RGB', img.size,\n",
    "#                                (255, 255, 255))  # Create white background\n",
    "#         background.paste(img, mask=img.split()[3])  # Use alpha channel as mask\n",
    "#         return background\n",
    "#     return img  # Return unchanged if no alpha channel\n",
    "\n",
    "# # Process images and masks\n",
    "# images = [remove_alpha(img) for img in tqdm(images)]\n",
    "# masks = [remove_alpha(msk) for msk in tqdm(masks)]\n",
    "\n",
    "# cropped_images = []\n",
    "# cropped_masks = []\n",
    "# for image, mask in zip(images, masks):\n",
    "\n",
    "#     cropped, cropped_mask = crop_around_center(image, mask, crop_size=200 * 3)\n",
    "\n",
    "#     cropped_images.append(cropped)\n",
    "#     cropped_masks.append(cropped_mask)\n",
    "\n",
    "train_dataset = ImageDataset(images=images[:400],\n",
    "                             masks=masks[:400],\n",
    "                             transform=True)\n",
    "val_dataset = ImageDataset(images=images[400:800], masks=masks[400:800])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_dataloader\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision import transforms as T\n",
    "import torchvision.transforms.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m im, gt \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_dataloader\u001b[49m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "for im, gt in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet152\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=2,\n",
    ")\n",
    "model.to(device)\n",
    "lr = 5e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def validate(model, val_dataloader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img_batch, gt_msk_batch in val_dataloader:\n",
    "\n",
    "            img_batch = img_batch.to(device)\n",
    "            gt_msk_batch = gt_msk_batch.to(device)\n",
    "\n",
    "            pred_mask = model(img_batch)\n",
    "\n",
    "            loss = dice_loss(torch.sigmoid(pred_mask), gt_msk_batch)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    mean_val_loss = val_loss / len(val_dataloader)\n",
    "    return mean_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2251228/3522360610.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m----> 2\u001b[0m     progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[43mtrain_dataloader\u001b[49m, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader))\n\u001b[1;32m      4\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m img_batch, gt_msk_batch \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    progress_bar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "\n",
    "    train_loss = 0\n",
    "    for img_batch, gt_msk_batch in progress_bar:\n",
    "        img_batch = img_batch.to(device)\n",
    "        gt_msk_batch = gt_msk_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            pred_mask = model(img_batch)\n",
    "\n",
    "            loss = dice_loss(torch.sigmoid(pred_mask), gt_msk_batch)\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        progress_bar.set_description(str(loss.item()))\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    val_loss = validate(model, val_dataloader)\n",
    "\n",
    "    print(\n",
    "        f'Epoch {epoch+1} | TrainLoss: {train_loss/len(train_dataloader)} ValLoss: {val_loss}'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mimg_batch\u001b[49m[\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m0\u001b[39m,:]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img_batch' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(img_batch[10, 0, :].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_mask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 40\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m iou, dice\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# # Example usage\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# pred_mask = torch.randint(0, 2, (256, 256),\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#                           dtype=torch.float32)  # Example predicted mask\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# gt_mask = torch.randint(0, 2, (256, 256),\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#                         dtype=torch.float32)  # Example ground truth mask\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m iou_fn, dice_fn \u001b[38;5;241m=\u001b[39m compute_iou_and_dice(torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[43mpred_mask\u001b[49m),\n\u001b[1;32m     41\u001b[0m                                        gt_msk_batch\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIoU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miou_fn\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Dice: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdice_fn\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred_mask' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def compute_iou_and_dice(pred: torch.Tensor,\n",
    "                         gt: torch.Tensor) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Compute IoU and Dice metrics for binary segmentation masks using PyTorch.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): Predicted binary mask (0 or 1), shape (H, W).\n",
    "        gt (torch.Tensor): Ground truth binary mask (0 or 1), shape (H, W).\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, float]: IoU and Dice scores.\n",
    "    \"\"\"\n",
    "    # Ensure binary masks (threshold at 0.5 for soft predictions)\n",
    "    pred = (pred > 0.5).float()\n",
    "    gt = (gt > 0.5).float()\n",
    "\n",
    "    # Compute intersection and union\n",
    "    intersection = torch.sum(pred * gt)\n",
    "    union = torch.sum(pred) + torch.sum(gt) - intersection\n",
    "\n",
    "    # Compute IoU\n",
    "    iou = (intersection / union).item() if union > 0 else 0.0\n",
    "\n",
    "    # Compute Dice coefficient\n",
    "    dice = (2 * intersection / (torch.sum(pred) + torch.sum(gt))).item() if (\n",
    "        torch.sum(pred) + torch.sum(gt)) > 0 else 0.0\n",
    "\n",
    "    return iou, dice\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# pred_mask = torch.randint(0, 2, (256, 256),\n",
    "#                           dtype=torch.float32)  # Example predicted mask\n",
    "# gt_mask = torch.randint(0, 2, (256, 256),\n",
    "#                         dtype=torch.float32)  # Example ground truth mask\n",
    "\n",
    "iou_fn, dice_fn = compute_iou_and_dice(torch.sigmoid(pred_mask),\n",
    "                                       gt_msk_batch.to(device))\n",
    "print(f\"IoU: {iou_fn:.4f}, Dice: {dice_fn:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    \"/home/tsakalis/ntua/phd/cellforge/cellforge/model_weights/big_good_pn_model.pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=\"resnext101_32x48d\",\n",
    "    encoder_weights=\"instagram\",\n",
    "    in_channels=3,\n",
    "    classes=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unet(\n",
       "  (encoder): ResNetEncoder(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (6): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (7): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (8): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (9): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (10): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (11): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (12): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (13): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (14): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (15): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (16): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (17): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (18): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (19): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (20): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (21): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (22): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(6144, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 12288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(12288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(12288, 12288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(12288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(12288, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 12288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(12288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(12288, 12288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(12288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(12288, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 12288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(12288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(12288, 12288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(12288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(12288, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): UnetDecoder(\n",
       "    (center): Identity()\n",
       "    (blocks): ModuleList(\n",
       "      (0): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(3072, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(768, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(16, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Identity()\n",
       "    (2): Activation(\n",
       "      (activation): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        \"/home/tsakalis/ntua/phd/cellforge/cellforge/model_weights/pronuclei.pt\",\n",
    "        weights_only=True))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─ResNetEncoder: 1-1                     [-1, 3, 224, 224]         --\n",
      "|    └─Conv2d: 2-1                       [-1, 64, 112, 112]        9,408\n",
      "|    └─BatchNorm2d: 2-2                  [-1, 64, 112, 112]        128\n",
      "|    └─ReLU: 2-3                         [-1, 64, 112, 112]        --\n",
      "|    └─MaxPool2d: 2-4                    [-1, 64, 56, 56]          --\n",
      "|    └─Sequential: 2-5                   [-1, 256, 56, 56]         --\n",
      "|    |    └─Bottleneck: 3-1              [-1, 256, 56, 56]         1,178,624\n",
      "|    |    └─Bottleneck: 3-2              [-1, 256, 56, 56]         1,456,640\n",
      "|    |    └─Bottleneck: 3-3              [-1, 256, 56, 56]         1,456,640\n",
      "|    └─Sequential: 2-6                   [-1, 512, 28, 28]         --\n",
      "|    |    └─Bottleneck: 3-4              [-1, 512, 28, 28]         5,158,912\n",
      "|    |    └─Bottleneck: 3-5              [-1, 512, 28, 28]         5,813,248\n",
      "|    |    └─Bottleneck: 3-6              [-1, 512, 28, 28]         5,813,248\n",
      "|    |    └─Bottleneck: 3-7              [-1, 512, 28, 28]         5,813,248\n",
      "|    └─Sequential: 2-7                   [-1, 1024, 14, 14]        --\n",
      "|    |    └─Bottleneck: 3-8              [-1, 1024, 14, 14]        20,606,976\n",
      "|    |    └─Bottleneck: 3-9              [-1, 1024, 14, 14]        23,226,368\n",
      "|    |    └─Bottleneck: 3-10             [-1, 1024, 14, 14]        23,226,368\n",
      "|    |    └─Bottleneck: 3-11             [-1, 1024, 14, 14]        23,226,368\n",
      "|    |    └─Bottleneck: 3-12             [-1, 1024, 14, 14]        23,226,368\n",
      "|    |    └─Bottleneck: 3-13             [-1, 1024, 14, 14]        23,226,368\n",
      "|    |    └─Bottleneck: 3-14             [-1, 1024, 14, 14]        23,226,368\n",
      "|    |    └─Bottleneck: 3-15             [-1, 1024, 14, 14]        23,226,368\n",
      "|    |    └─Bottleneck: 3-16             [-1, 1024, 14, 14]        23,226,368\n",
      "|    |    └─Bottleneck: 3-17             [-1, 1024, 14, 14]        23,226,368\n",
      "|    |    └─Bottleneck: 3-18             [-1, 1024, 14, 14]        23,226,368\n",
      "|    |    └─Bottleneck: 3-19             [-1, 1024, 14, 14]        23,226,368\n",
      "|    |    └─Bottleneck: 3-20             [-1, 1024, 14, 14]        23,226,368\n",
      "|    |    └─Bottleneck: 3-21             [-1, 1024, 14, 14]        23,226,368\n",
      "|    |    └─Bottleneck: 3-22             [-1, 1024, 14, 14]        23,226,368\n",
      "|    |    └─Bottleneck: 3-23             [-1, 1024, 14, 14]        23,226,368\n",
      "|    |    └─Bottleneck: 3-24             [-1, 1024, 14, 14]        23,226,368\n",
      "|    |    └─Bottleneck: 3-25             [-1, 1024, 14, 14]        23,226,368\n",
      "|    |    └─Bottleneck: 3-26             [-1, 1024, 14, 14]        23,226,368\n",
      "|    |    └─Bottleneck: 3-27             [-1, 1024, 14, 14]        23,226,368\n",
      "|    |    └─Bottleneck: 3-28             [-1, 1024, 14, 14]        23,226,368\n",
      "|    |    └─Bottleneck: 3-29             [-1, 1024, 14, 14]        23,226,368\n",
      "|    |    └─Bottleneck: 3-30             [-1, 1024, 14, 14]        23,226,368\n",
      "|    └─Sequential: 2-8                   [-1, 2048, 7, 7]          --\n",
      "|    |    └─Bottleneck: 3-31             [-1, 2048, 7, 7]          82,370,560\n",
      "|    |    └─Bottleneck: 3-32             [-1, 2048, 7, 7]          92,852,224\n",
      "|    |    └─Bottleneck: 3-33             [-1, 2048, 7, 7]          92,852,224\n",
      "├─UnetDecoder: 1-2                       [-1, 16, 224, 224]        --\n",
      "|    └─Identity: 2-9                     [-1, 2048, 7, 7]          --\n",
      "|    └─ModuleList: 2                     []                        --\n",
      "|    |    └─DecoderBlock: 3-34           [-1, 256, 14, 14]         7,668,736\n",
      "|    |    └─DecoderBlock: 3-35           [-1, 128, 28, 28]         1,032,704\n",
      "|    |    └─DecoderBlock: 3-36           [-1, 64, 56, 56]          258,304\n",
      "|    |    └─DecoderBlock: 3-37           [-1, 32, 112, 112]        46,208\n",
      "|    |    └─DecoderBlock: 3-38           [-1, 16, 224, 224]        6,976\n",
      "├─SegmentationHead: 1-3                  [-1, 2, 224, 224]         --\n",
      "|    └─Conv2d: 2-10                      [-1, 2, 224, 224]         290\n",
      "|    └─Identity: 2-11                    [-1, 2, 224, 224]         --\n",
      "|    └─Activation: 2-12                  [-1, 2, 224, 224]         --\n",
      "|    |    └─Identity: 3-39               [-1, 2, 224, 224]         --\n",
      "==========================================================================================\n",
      "Total params: 835,375,394\n",
      "Trainable params: 835,375,394\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 155.47\n",
      "==========================================================================================\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 1977.61\n",
      "Params size (MB): 3186.70\n",
      "Estimated Total Size (MB): 5164.89\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─ResNetEncoder: 1-1                     [-1, 3, 224, 224]         --\n",
       "|    └─Conv2d: 2-1                       [-1, 64, 112, 112]        9,408\n",
       "|    └─BatchNorm2d: 2-2                  [-1, 64, 112, 112]        128\n",
       "|    └─ReLU: 2-3                         [-1, 64, 112, 112]        --\n",
       "|    └─MaxPool2d: 2-4                    [-1, 64, 56, 56]          --\n",
       "|    └─Sequential: 2-5                   [-1, 256, 56, 56]         --\n",
       "|    |    └─Bottleneck: 3-1              [-1, 256, 56, 56]         1,178,624\n",
       "|    |    └─Bottleneck: 3-2              [-1, 256, 56, 56]         1,456,640\n",
       "|    |    └─Bottleneck: 3-3              [-1, 256, 56, 56]         1,456,640\n",
       "|    └─Sequential: 2-6                   [-1, 512, 28, 28]         --\n",
       "|    |    └─Bottleneck: 3-4              [-1, 512, 28, 28]         5,158,912\n",
       "|    |    └─Bottleneck: 3-5              [-1, 512, 28, 28]         5,813,248\n",
       "|    |    └─Bottleneck: 3-6              [-1, 512, 28, 28]         5,813,248\n",
       "|    |    └─Bottleneck: 3-7              [-1, 512, 28, 28]         5,813,248\n",
       "|    └─Sequential: 2-7                   [-1, 1024, 14, 14]        --\n",
       "|    |    └─Bottleneck: 3-8              [-1, 1024, 14, 14]        20,606,976\n",
       "|    |    └─Bottleneck: 3-9              [-1, 1024, 14, 14]        23,226,368\n",
       "|    |    └─Bottleneck: 3-10             [-1, 1024, 14, 14]        23,226,368\n",
       "|    |    └─Bottleneck: 3-11             [-1, 1024, 14, 14]        23,226,368\n",
       "|    |    └─Bottleneck: 3-12             [-1, 1024, 14, 14]        23,226,368\n",
       "|    |    └─Bottleneck: 3-13             [-1, 1024, 14, 14]        23,226,368\n",
       "|    |    └─Bottleneck: 3-14             [-1, 1024, 14, 14]        23,226,368\n",
       "|    |    └─Bottleneck: 3-15             [-1, 1024, 14, 14]        23,226,368\n",
       "|    |    └─Bottleneck: 3-16             [-1, 1024, 14, 14]        23,226,368\n",
       "|    |    └─Bottleneck: 3-17             [-1, 1024, 14, 14]        23,226,368\n",
       "|    |    └─Bottleneck: 3-18             [-1, 1024, 14, 14]        23,226,368\n",
       "|    |    └─Bottleneck: 3-19             [-1, 1024, 14, 14]        23,226,368\n",
       "|    |    └─Bottleneck: 3-20             [-1, 1024, 14, 14]        23,226,368\n",
       "|    |    └─Bottleneck: 3-21             [-1, 1024, 14, 14]        23,226,368\n",
       "|    |    └─Bottleneck: 3-22             [-1, 1024, 14, 14]        23,226,368\n",
       "|    |    └─Bottleneck: 3-23             [-1, 1024, 14, 14]        23,226,368\n",
       "|    |    └─Bottleneck: 3-24             [-1, 1024, 14, 14]        23,226,368\n",
       "|    |    └─Bottleneck: 3-25             [-1, 1024, 14, 14]        23,226,368\n",
       "|    |    └─Bottleneck: 3-26             [-1, 1024, 14, 14]        23,226,368\n",
       "|    |    └─Bottleneck: 3-27             [-1, 1024, 14, 14]        23,226,368\n",
       "|    |    └─Bottleneck: 3-28             [-1, 1024, 14, 14]        23,226,368\n",
       "|    |    └─Bottleneck: 3-29             [-1, 1024, 14, 14]        23,226,368\n",
       "|    |    └─Bottleneck: 3-30             [-1, 1024, 14, 14]        23,226,368\n",
       "|    └─Sequential: 2-8                   [-1, 2048, 7, 7]          --\n",
       "|    |    └─Bottleneck: 3-31             [-1, 2048, 7, 7]          82,370,560\n",
       "|    |    └─Bottleneck: 3-32             [-1, 2048, 7, 7]          92,852,224\n",
       "|    |    └─Bottleneck: 3-33             [-1, 2048, 7, 7]          92,852,224\n",
       "├─UnetDecoder: 1-2                       [-1, 16, 224, 224]        --\n",
       "|    └─Identity: 2-9                     [-1, 2048, 7, 7]          --\n",
       "|    └─ModuleList: 2                     []                        --\n",
       "|    |    └─DecoderBlock: 3-34           [-1, 256, 14, 14]         7,668,736\n",
       "|    |    └─DecoderBlock: 3-35           [-1, 128, 28, 28]         1,032,704\n",
       "|    |    └─DecoderBlock: 3-36           [-1, 64, 56, 56]          258,304\n",
       "|    |    └─DecoderBlock: 3-37           [-1, 32, 112, 112]        46,208\n",
       "|    |    └─DecoderBlock: 3-38           [-1, 16, 224, 224]        6,976\n",
       "├─SegmentationHead: 1-3                  [-1, 2, 224, 224]         --\n",
       "|    └─Conv2d: 2-10                      [-1, 2, 224, 224]         290\n",
       "|    └─Identity: 2-11                    [-1, 2, 224, 224]         --\n",
       "|    └─Activation: 2-12                  [-1, 2, 224, 224]         --\n",
       "|    |    └─Identity: 3-39               [-1, 2, 224, 224]         --\n",
       "==========================================================================================\n",
       "Total params: 835,375,394\n",
       "Trainable params: 835,375,394\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 155.47\n",
       "==========================================================================================\n",
       "Input size (MB): 0.57\n",
       "Forward/backward pass size (MB): 1977.61\n",
       "Params size (MB): 3186.70\n",
       "Estimated Total Size (MB): 5164.89\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m pred_masks_all \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m all_images \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_batch, gt_msk_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mval_dataloader\u001b[49m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[1;32m     11\u001b[0m         pred_mask \u001b[38;5;241m=\u001b[39m model(img_batch\u001b[38;5;241m.\u001b[39mto(device))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "gt_masks_all = []\n",
    "pred_masks_all = []\n",
    "\n",
    "all_images = []\n",
    "\n",
    "for img_batch, gt_msk_batch in val_dataloader:\n",
    "\n",
    "    with autocast():\n",
    "        pred_mask = model(img_batch.to(device))\n",
    "    all_images.append(img_batch)\n",
    "    pred_masks_all.append(pred_mask.to('cpu'))\n",
    "    gt_masks_all.append(gt_msk_batch.to('cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import JaccardIndex, Dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'JaccardIndex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m iou_fn \u001b[38;5;241m=\u001b[39m \u001b[43mJaccardIndex\u001b[49m(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.90\u001b[39m)\n\u001b[1;32m      2\u001b[0m dice_fn \u001b[38;5;241m=\u001b[39m Dice(threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.90\u001b[39m)\n\u001b[1;32m      4\u001b[0m iou_fn(torch\u001b[38;5;241m.\u001b[39msigmoid(torch\u001b[38;5;241m.\u001b[39mvstack(pred_masks_all)), torch\u001b[38;5;241m.\u001b[39mvstack(gt_masks_all))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'JaccardIndex' is not defined"
     ]
    }
   ],
   "source": [
    "iou_fn = JaccardIndex(task='binary', threshold=0.90)\n",
    "dice_fn = Dice(threshold=0.90)\n",
    "\n",
    "iou_fn(torch.sigmoid(torch.vstack(pred_masks_all)), torch.vstack(gt_masks_all))\n",
    "\n",
    "dice_fn(\n",
    "    torch.sigmoid(torch.vstack(pred_masks_all)),\n",
    "    torch.vstack(gt_masks_all).long(),\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"IoU: {iou_fn(torch.sigmoid(torch.vstack(pred_masks_all)), torch.vstack(gt_masks_all)):.3f}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Dice Coeff: {dice_fn(torch.sigmoid(torch.vstack(pred_masks_all)), torch.vstack(gt_masks_all).long()):.3f}\"\n",
    ")\n",
    "\n",
    "sample_pred_mask = torch.sigmoid(pred_masks_all[0][3][0].detach())\n",
    "\n",
    "sample_gt_mask = gt_masks_all[0][3][0].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage.morphology import binary_dilation\n",
    "\n",
    "slide_id = 'D2016.10.18_S1418_I149_6'\n",
    "sample_path = Path(\"/home/tsakalis/ntua/phd/cellforge/cellforge/data/\")\n",
    "\n",
    "\n",
    "def inference_whole_slide(model, slide_pth: Path, max_frame: int):\n",
    "\n",
    "    image_file_paths = sorted(list(slide_pth.glob('*.jpg')),\n",
    "                              key=lambda x: int(x.stem))[:max_frame]\n",
    "\n",
    "    images = [Image.open(img_path) for img_path in tqdm(image_file_paths)]\n",
    "\n",
    "    val_dataset = ImageDataset(images=images, masks=images)\n",
    "\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "    model.eval()\n",
    "    from torch.cuda.amp import autocast\n",
    "\n",
    "    all_masks = []\n",
    "    for inpt_images, _ in val_dataloader:\n",
    "        with torch.no_grad():\n",
    "            with autocast():\n",
    "                pred_mask = model(inpt_images.to(device))\n",
    "                masks = torch.sigmoid(pred_mask).cpu().numpy()\n",
    "                all_masks.extend([msk for msk in masks])\n",
    "\n",
    "    pn_size = []\n",
    "    final_images = []\n",
    "    upscaled_masks = []\n",
    "    isolated_pns = []\n",
    "    for pil_img, mask in zip(images[:], all_masks[:]):\n",
    "        # Ensure the mask is 2D by removing extra dimensions\n",
    "        # pil_img = pil_img.resize((224, 224), Image.Resampling.LANCZOS)\n",
    "        image_ar = np.stack(3 * [np.array(pil_img)])\n",
    "\n",
    "        upscaled_mask = cv2.resize(mask[0].astype(np.uint8), (500, 500),\n",
    "                                   interpolation=cv2.INTER_NEAREST)\n",
    "        pn_size.append(upscaled_mask.sum())\n",
    "\n",
    "        upscaled_masks.append(upscaled_mask)\n",
    "        image_pn_isolated = image_ar.copy()\n",
    "        image_pn_isolated[:, ~upscaled_mask.astype(bool)] = 0\n",
    "        isolated_pns.append(image_pn_isolated.transpose(1, 2, 0))\n",
    "        image_ar[2, upscaled_mask.astype(bool)] = 240\n",
    "\n",
    "        final_images.append(Image.fromarray(image_ar.transpose(1, 2, 0)))\n",
    "\n",
    "    return final_images, upscaled_masks, pn_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def video_writer_context(output_path, frame_height, frame_width, fps=5):\n",
    "    # Create a dummy figure to get graph size\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot([0, 1], [0, 1])\n",
    "    fig.canvas.draw()\n",
    "    graph_h, graph_w = fig.canvas.get_width_height()\n",
    "    plt.close(fig)\n",
    "\n",
    "    scale = frame_height / graph_h\n",
    "    graph_resized_width = int(graph_w * scale)\n",
    "    output_size = (frame_width + graph_resized_width, frame_height)\n",
    "\n",
    "    output = cv2.VideoWriter(str(output_path), cv2.VideoWriter_fourcc(*'XVID'),\n",
    "                             fps, output_size)\n",
    "\n",
    "    try:\n",
    "        yield output, graph_resized_width\n",
    "    finally:\n",
    "        output.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def generate_video(slide_images,\n",
    "                   slide_masks,\n",
    "                   output_path,\n",
    "                   frame_height=500,\n",
    "                   frame_width=500):\n",
    "    pn_size = []\n",
    "\n",
    "    with video_writer_context(output_path, frame_height,\n",
    "                              frame_width) as (output, graph_resized_width):\n",
    "        for frame_idx, frame in enumerate(slide_images):\n",
    "            pn_size.append(slide_masks[frame_idx].sum())\n",
    "            x = np.arange(start=0, stop=frame_idx + 1, step=1)\n",
    "\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.plot(x, pn_size)\n",
    "            ax.set_title(f\"Accumulated PN Size (Frame {frame_idx})\")\n",
    "            ax.set_xlabel('Frame')\n",
    "            ax.set_ylabel('Accumulated Area')\n",
    "            fig.tight_layout()\n",
    "            fig.canvas.draw()\n",
    "\n",
    "            plot_img = np.asarray(fig.canvas.buffer_rgba())[:, :, :3]\n",
    "            plt.close(fig)\n",
    "\n",
    "            plot_resized = cv2.resize(plot_img,\n",
    "                                      (graph_resized_width, frame_height))\n",
    "            plot_bgr = cv2.cvtColor(plot_resized, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            combined = np.hstack((np.array(frame), plot_bgr))\n",
    "\n",
    "            output.write(combined)\n",
    "            cv2.imshow(\"output\", combined)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_video(slide_images, slide_masks, sample_idx):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:00<00:00, 41328.62it/s]\n",
      "/tmp/ipykernel_2251228/4218687623.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 150/150 [00:00<00:00, 36436.30it/s]\n",
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 150/150 [00:00<00:00, 39184.45it/s]\n",
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 150/150 [00:00<00:00, 40684.53it/s]\n",
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    }
   ],
   "source": [
    "sample_ids = [\n",
    "    'D2016.07.08_S1366_I149_11', 'D2016.10.18_S1418_I149_6',\n",
    "    'D2016.10.18_S1418_I149_8', 'D2016.10.18_S1418_I149_11'\n",
    "]\n",
    "\n",
    "for sample_idx, sample_id in enumerate(sample_ids):\n",
    "\n",
    "    slide_images, slide_masks, pn_area = inference_whole_slide(\n",
    "        model, sample_path / sample_id, 150)\n",
    "    # output_path = Path(f\"/home/tsakalis/pn_samples/accumulated_pn_area_sample_whole{sample_idx}_multilabel.mp4\")\n",
    "    # generate_video(slide_images, slide_masks, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 560/560 [00:00<00:00, 40858.11it/s]\n"
     ]
    }
   ],
   "source": [
    "sample_path = Path(\n",
    "    \"/home/tsakalis/ntua/phd/cellforge/cellforge/data/D2016.10.18_S1418_I149_6\"\n",
    ")\n",
    "\n",
    "image_file_paths = sorted(list(sample_path.glob('*.jpg')),\n",
    "                          key=lambda x: int(x.stem))\n",
    "\n",
    "images = [Image.open(img_path) for img_path in tqdm(image_file_paths)]\n",
    "\n",
    "# def remove_alpha(img):\n",
    "#     if img.mode == 'RGBA':  # If image has an alpha channel\n",
    "#         background = Image.new('RGB', img.size,\n",
    "#                                (255, 255, 255))  # Create white background\n",
    "#         background.paste(img, mask=img.split()[3])  # Use alpha channel as mask\n",
    "#         return background\n",
    "#     return img  # Return unchanged if no alpha channel\n",
    "\n",
    "# # Process images and masks\n",
    "# images = [remove_alpha(img) for img in tqdm(images)]\n",
    "# masks = [remove_alpha(msk) for msk in tqdm(masks)]\n",
    "\n",
    "# cropped_images = []\n",
    "# cropped_masks = []\n",
    "# for image, mask in zip(images, masks):\n",
    "\n",
    "#     cropped, cropped_mask = crop_around_center(image, mask, crop_size=200 * 3)\n",
    "\n",
    "#     cropped_images.append(cropped)\n",
    "#     cropped_masks.append(cropped_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:00<00:00, 34734.48it/s]\n",
      "/tmp/ipykernel_2072954/2015305102.py:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x79c1454b5580>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjUAAAGdCAYAAADqsoKGAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAMHdJREFUeJzt3X9U1mWe//HXjfKrhBtJuYHEwDS18WdmiM6kHlnRzGKqXfXYag1TTQONhDMVfdOybQ+mo6dtc3UbU7NZo3FO2kZOG4OJOSKI6RZWlK1GGTeW5X0rJiFc3z88fXbvxB83qMjl83HO5xzu63pf1319rsM59+t8+NwfXMYYIwAAgA4upL0XAAAAcC4QagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAVujc3gu4UJqbm/Xll18qKipKLpervZcDAADOgjFGhw8fVmJiokJCTn8t5pIJNV9++aWSkpLaexkAAKAVPv/8c/Xo0eO0NZdMqImKipJ0YlOio6PbeTUAAOBs+P1+JSUlOZ/jp3PJhJof/uQUHR1NqAEAoIM5m1tHuFEYAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFghqFBTUFCg4cOHKyoqSnFxccrMzFR1dfVpx+zevVu33367kpOT5XK59Mwzz7RYt2TJEiUnJysiIkKpqamqqKgI6D927Jiys7N1xRVXqEuXLrr99ttVV1cXzPIBAIDFggo1paWlys7O1rZt21RcXKzGxkaNHz9e9fX1pxxz9OhR9erVS/Pnz1d8fHyLNa+88ory8vL0+OOP691339XgwYOVkZGhAwcOODUPPvigXn/9da1du1alpaX68ssvddtttwWzfAAAYDGXMca0dvBXX32luLg4lZaW6sYbbzxjfXJysnJzc5WbmxvQnpqaquHDh+u5556TJDU3NyspKUkPPPCAHnnkEfl8PnXv3l1r1qzRHXfcIUn66KOP1L9/f5WVlWnEiBFnfG+/3y+32y2fz8f/fgIAoIMI5vO7TffU+Hw+SVJsbGyr5/j++++1Y8cOpaen/++iQkKUnp6usrIySdKOHTvU2NgYUNOvXz/17NnTqfmxhoYG+f3+gAMAANir1aGmublZubm5GjVqlAYMGNDqBXz99ddqamqSx+MJaPd4PPJ6vZIkr9ersLAwxcTEnLLmxwoKCuR2u50jKSmp1WsEAAAXv1aHmuzsbFVVVamwsPBcruecyc/Pl8/nc47PP/+8vZcEAADOo86tGZSTk6OioiJt3rxZPXr0aNMCunXrpk6dOp30Taa6ujrnxuL4+Hh9//33OnToUMDVmv9b82Ph4eEKDw9v09oAAEDHEdSVGmOMcnJytG7dOm3cuFEpKSltXkBYWJiGDRumkpISp625uVklJSVKS0uTJA0bNkyhoaEBNdXV1aqpqXFqAADApS2oKzXZ2dlas2aNXnvtNUVFRTn3s7jdbkVGRkqSZsyYoSuvvFIFBQWSTtwI/MEHHzg/79+/X7t27VKXLl3Uu3dvSVJeXp5mzpyp66+/XjfccIOeeeYZ1dfX6+6773bmz8rKUl5enmJjYxUdHa0HHnhAaWlpZ/XNJwAAYL+gvtLtcrlabF+5cqXuuusuSdKYMWOUnJysVatWSZL27dvX4hWd0aNHa9OmTc7r5557TgsXLpTX69WQIUP07LPPKjU11ek/duyYZs+erZdfflkNDQ3KyMjQv/3bv53yz08/xle6AQDoeIL5/G7Tc2o6EkINAAAdzwV7Tg0AAMDFglADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGCFoEJNQUGBhg8frqioKMXFxSkzM1PV1dVnHLd27Vr169dPERERGjhwoDZs2BDQ73K5WjwWLlzo1CQnJ5/UP3/+/GCWDwAALBZUqCktLVV2dra2bdum4uJiNTY2avz48aqvrz/lmK1bt2ratGnKysrSzp07lZmZqczMTFVVVTk1tbW1AceKFSvkcrl0++23B8z15JNPBtQ98MADQZ4uAACwlcsYY1o7+KuvvlJcXJxKS0t14403tlgzZcoU1dfXq6ioyGkbMWKEhgwZomXLlrU4JjMzU4cPH1ZJSYnTlpycrNzcXOXm5rZqrX6/X263Wz6fT9HR0a2aAwAAXFjBfH636Z4an88nSYqNjT1lTVlZmdLT0wPaMjIyVFZW1mJ9XV2d3njjDWVlZZ3UN3/+fF1xxRUaOnSoFi5cqOPHj5/yfRsaGuT3+wMOAABgr86tHdjc3Kzc3FyNGjVKAwYMOGWd1+uVx+MJaPN4PPJ6vS3Wv/jii4qKitJtt90W0P6b3/xG1113nWJjY7V161bl5+ertrZWixcvbnGegoICzZs3L8izAgAAHVWrQ012draqqqq0ZcuWc7kerVixQtOnT1dERERAe15envPzoEGDFBYWpvvuu08FBQUKDw8/aZ78/PyAMX6/X0lJSed0rQAA4OLRqlCTk5OjoqIibd68WT169DhtbXx8vOrq6gLa6urqFB8ff1LtO++8o+rqar3yyitnXENqaqqOHz+uffv2qW/fvif1h4eHtxh2AACAnYK6p8YYo5ycHK1bt04bN25USkrKGcekpaUF3PArScXFxUpLSzup9oUXXtCwYcM0ePDgM867a9cuhYSEKC4u7uxPAAAAWCuoKzXZ2dlas2aNXnvtNUVFRTn3xbjdbkVGRkqSZsyYoSuvvFIFBQWSpFmzZmn06NFatGiRJk2apMLCQlVWVur5558PmNvv92vt2rVatGjRSe9bVlam8vJyjR07VlFRUSorK9ODDz6oO++8U127dm3ViQMAALsEFWqWLl0qSRozZkxA+8qVK3XXXXdJkmpqahQS8r8XgEaOHKk1a9boscce06OPPqo+ffpo/fr1J91cXFhYKGOMpk2bdtL7hoeHq7CwUE888YQaGhqUkpKiBx98MOCeGQAAcGlr03NqOhKeUwMAQMdzwZ5TAwAAcLEg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWCGoUFNQUKDhw4crKipKcXFxyszMVHV19RnHrV27Vv369VNERIQGDhyoDRs2BPTfddddcrlcAceECRMCar755htNnz5d0dHRiomJUVZWlo4cORLM8gEAgMWCCjWlpaXKzs7Wtm3bVFxcrMbGRo0fP1719fWnHLN161ZNmzZNWVlZ2rlzpzIzM5WZmamqqqqAugkTJqi2ttY5Xn755YD+6dOna/fu3SouLlZRUZE2b96se++9N5jlAwAAi7mMMaa1g7/66ivFxcWptLRUN954Y4s1U6ZMUX19vYqKipy2ESNGaMiQIVq2bJmkE1dqDh06pPXr17c4x4cffqhrr71W27dv1/XXXy9JevPNN3XTTTfpiy++UGJi4hnX6vf75Xa75fP5FB0dHeSZAgCA9hDM53eb7qnx+XySpNjY2FPWlJWVKT09PaAtIyNDZWVlAW2bNm1SXFyc+vbtq/vvv18HDx4MmCMmJsYJNJKUnp6ukJAQlZeXt/i+DQ0N8vv9AQcAALBXq0NNc3OzcnNzNWrUKA0YMOCUdV6vVx6PJ6DN4/HI6/U6rydMmKDVq1erpKRETz/9tEpLSzVx4kQ1NTU5c8TFxQXM0blzZ8XGxgbM838VFBTI7XY7R1JSUmtPFQAAdACdWzswOztbVVVV2rJlS5sXMXXqVOfngQMHatCgQbr66qu1adMmjRs3rlVz5ufnKy8vz3nt9/sJNgAAWKxVV2pycnJUVFSkt99+Wz169DhtbXx8vOrq6gLa6urqFB8ff8oxvXr1Urdu3bRnzx5njgMHDgTUHD9+XN98880p5wkPD1d0dHTAAQAA7BVUqDHGKCcnR+vWrdPGjRuVkpJyxjFpaWkqKSkJaCsuLlZaWtopx3zxxRc6ePCgEhISnDkOHTqkHTt2ODUbN25Uc3OzUlNTgzkFAABgqaBCTXZ2tv74xz9qzZo1ioqKktfrldfr1XfffefUzJgxQ/n5+c7rWbNm6c0339SiRYv00Ucf6YknnlBlZaVycnIkSUeOHNHvfvc7bdu2Tfv27VNJSYluvfVW9e7dWxkZGZKk/v37a8KECbrnnntUUVGhv/3tb8rJydHUqVPP6ptPAADAfkGFmqVLl8rn82nMmDFKSEhwjldeecWpqampUW1trfN65MiRWrNmjZ5//nkNHjxYf/7zn7V+/Xrn5uJOnTrpvffe0y233KJrrrlGWVlZGjZsmN555x2Fh4c78/zHf/yH+vXrp3Hjxummm27ST3/6Uz3//PNtPX8AAGCJNj2npiPhOTUAAHQ8F+w5NQAAABcLQg0AALACoQYAAFiBUAMAAKxAqAEAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYg1AAAACsQagAAgBWCCjUFBQUaPny4oqKiFBcXp8zMTFVXV59x3Nq1a9WvXz9FRERo4MCB2rBhg9PX2Niohx9+WAMHDtTll1+uxMREzZgxQ19++WXAHMnJyXK5XAHH/Pnzg1k+AACwWFChprS0VNnZ2dq2bZuKi4vV2Nio8ePHq76+/pRjtm7dqmnTpikrK0s7d+5UZmamMjMzVVVVJUk6evSo3n33Xc2ZM0fvvvuuXn31VVVXV+uWW245aa4nn3xStbW1zvHAAw8EeboAAMBWLmOMae3gr776SnFxcSotLdWNN97YYs2UKVNUX1+voqIip23EiBEaMmSIli1b1uKY7du364YbbtBnn32mnj17SjpxpSY3N1e5ubmtWqvf75fb7ZbP51N0dHSr5gAAABdWMJ/fbbqnxufzSZJiY2NPWVNWVqb09PSAtoyMDJWVlZ12XpfLpZiYmID2+fPn64orrtDQoUO1cOFCHT9+/JRzNDQ0yO/3BxwAAMBenVs7sLm5Wbm5uRo1apQGDBhwyjqv1yuPxxPQ5vF45PV6W6w/duyYHn74YU2bNi0gkf3mN7/Rddddp9jYWG3dulX5+fmqra3V4sWLW5ynoKBA8+bNa8WZAQCAjqjVoSY7O1tVVVXasmXLOVtMY2Oj/uEf/kHGGC1dujSgLy8vz/l50KBBCgsL03333aeCggKFh4efNFd+fn7AGL/fr6SkpHO2VgAAcHFpVajJyclRUVGRNm/erB49epy2Nj4+XnV1dQFtdXV1io+PD2j7IdB89tln2rhx4xn/bpaamqrjx49r37596tu370n94eHhLYYdAABgp6DuqTHGKCcnR+vWrdPGjRuVkpJyxjFpaWkqKSkJaCsuLlZaWprz+odA88knn+ivf/2rrrjiijPOu2vXLoWEhCguLi6YUwAAAJYK6kpNdna21qxZo9dee01RUVHOfTFut1uRkZGSpBkzZujKK69UQUGBJGnWrFkaPXq0Fi1apEmTJqmwsFCVlZV6/vnnJZ0INHfccYfeffddFRUVqampyZk3NjZWYWFhKisrU3l5ucaOHauoqCiVlZXpwQcf1J133qmuXbues80AAAAdV1Bf6Xa5XC22r1y5UnfddZckacyYMUpOTtaqVauc/rVr1+qxxx7Tvn371KdPHy1YsEA33XSTJGnfvn2nvOLz9ttva8yYMXr33Xf161//Wh999JEaGhqUkpKif/zHf1ReXt5Z/4mJr3QDANDxBPP53abn1HQkhBoAADqeC/acGgAAgIsFoQYAAFiBUAMAAKxAqAEAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYg1AAAACsQagAAgBUINQAAwApBhZqCggINHz5cUVFRiouLU2Zmpqqrq884bu3aterXr58iIiI0cOBAbdiwIaDfGKO5c+cqISFBkZGRSk9P1yeffBJQ880332j69OmKjo5WTEyMsrKydOTIkWCWDwAALBZUqCktLVV2dra2bdum4uJiNTY2avz48aqvrz/lmK1bt2ratGnKysrSzp07lZmZqczMTFVVVTk1CxYs0LPPPqtly5apvLxcl19+uTIyMnTs2DGnZvr06dq9e7eKi4tVVFSkzZs36957723FKQMAABu5jDGmtYO/+uorxcXFqbS0VDfeeGOLNVOmTFF9fb2KioqcthEjRmjIkCFatmyZjDFKTEzU7Nmz9dvf/laS5PP55PF4tGrVKk2dOlUffvihrr32Wm3fvl3XX3+9JOnNN9/UTTfdpC+++EKJiYlnXKvf75fb7ZbP51N0dHRrT/kkxhh919h0zuYDAKAjiwztJJfLdc7mC+bzu3Nb3sjn80mSYmNjT1lTVlamvLy8gLaMjAytX79ekrR37155vV6lp6c7/W63W6mpqSorK9PUqVNVVlammJgYJ9BIUnp6ukJCQlReXq6f//znJ71vQ0ODGhoanNd+v79V53gm3zU26dq5/3Ve5gYAoKP54MkMXRbWpnjRaq2+Ubi5uVm5ubkaNWqUBgwYcMo6r9crj8cT0ObxeOT1ep3+H9pOVxMXFxfQ37lzZ8XGxjo1P1ZQUCC32+0cSUlJwZ0gAADoUFodpbKzs1VVVaUtW7acy/WcM/n5+QFXiPx+/3kJNpGhnfTBkxnnfF4AADqiyNBO7fberQo1OTk5zs26PXr0OG1tfHy86urqAtrq6uoUHx/v9P/QlpCQEFAzZMgQp+bAgQMBcxw/flzffPONM/7HwsPDFR4eHtR5tYbL5Wq3y2wAAOB/BfXnJ2OMcnJytG7dOm3cuFEpKSlnHJOWlqaSkpKAtuLiYqWlpUmSUlJSFB8fH1Dj9/tVXl7u1KSlpenQoUPasWOHU7Nx40Y1NzcrNTU1mFMAAACWCuoSQ3Z2ttasWaPXXntNUVFRzv0sbrdbkZGRkqQZM2boyiuvVEFBgSRp1qxZGj16tBYtWqRJkyapsLBQlZWVev755yWduNKRm5urp556Sn369FFKSormzJmjxMREZWZmSpL69++vCRMm6J577tGyZcvU2NionJwcTZ069ay++QQAAC4BJgiSWjxWrlzp1IwePdrMnDkzYNyf/vQnc80115iwsDDzk5/8xLzxxhsB/c3NzWbOnDnG4/GY8PBwM27cOFNdXR1Qc/DgQTNt2jTTpUsXEx0dbe6++25z+PDhs167z+czkozP5wvmlAEAQDsK5vO7Tc+p6UjO13NqAADA+RPM5zf/+wkAAFiBUAMAAKxAqAEAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYIWgQ83mzZs1efJkJSYmyuVyaf369Wccs2TJEvXv31+RkZHq27evVq9eHdA/ZswYuVyuk45JkyY5NXfddddJ/RMmTAh2+QAAwFKdgx1QX1+vwYMH6xe/+IVuu+22M9YvXbpU+fn5+sMf/qDhw4eroqJC99xzj7p27arJkydLkl599VV9//33zpiDBw9q8ODB+vu///uAuSZMmKCVK1c6r8PDw4NdPgAAsFTQoWbixImaOHHiWde/9NJLuu+++zRlyhRJUq9evbR9+3Y9/fTTTqiJjY0NGFNYWKjLLrvspFATHh6u+Pj4YJcMAAAuAef9npqGhgZFREQEtEVGRqqiokKNjY0tjnnhhRc0depUXX755QHtmzZtUlxcnPr27av7779fBw8ePO37+v3+gAMAANjrvIeajIwMLV++XDt27JAxRpWVlVq+fLkaGxv19ddfn1RfUVGhqqoq/fKXvwxonzBhglavXq2SkhI9/fTTKi0t1cSJE9XU1NTi+xYUFMjtdjtHUlLSeTk/AABwcXAZY0yrB7tcWrdunTIzM09Z89133yk7O1svvfSSjDHyeDy68847tWDBAnm9Xnk8noD6++67T2VlZXrvvfdO+97/8z//o6uvvlp//etfNW7cuJP6Gxoa1NDQ4Lz2+/1KSkqSz+dTdHR0cCcKAADahd/vl9vtPqvP7/N+pSYyMlIrVqzQ0aNHtW/fPtXU1Cg5OVlRUVHq3r17QG19fb0KCwuVlZV1xnl79eqlbt26ac+ePS32h4eHKzo6OuAAAAD2CvpG4dYKDQ1Vjx49JJ24Efjmm29WSEhgplq7dq0aGhp05513nnG+L774QgcPHlRCQsJ5WS8AAOhYgg41R44cCbg6snfvXu3atUuxsbHq2bOn8vPztX//fudZNB9//LEqKiqUmpqqb7/9VosXL1ZVVZVefPHFk+Z+4YUXlJmZqSuuuOKk95w3b55uv/12xcfH69NPP9VDDz2k3r17KyMjI9hTAAAAFgo61FRWVmrs2LHO67y8PEnSzJkztWrVKtXW1qqmpsbpb2pq0qJFi1RdXa3Q0FCNHTtWW7duVXJycsC81dXV2rJli956662T3rNTp05677339OKLL+rQoUNKTEzU+PHj9U//9E88qwYAAEhq443CHUkwNxoBAICLw0V1ozAAAMCFQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALBC0KFm8+bNmjx5shITE+VyubR+/fozjlmyZIn69++vyMhI9e3bV6tXrw7oX7VqlVwuV8ARERERUGOM0dy5c5WQkKDIyEilp6frk08+CXb5AADAUkGHmvr6eg0ePFhLliw5q/qlS5cqPz9fTzzxhHbv3q158+YpOztbr7/+ekBddHS0amtrneOzzz4L6F+wYIGeffZZLVu2TOXl5br88suVkZGhY8eOBXsKAADAQp2DHTBx4kRNnDjxrOtfeukl3XfffZoyZYokqVevXtq+fbuefvppTZ482alzuVyKj49vcQ5jjJ555hk99thjuvXWWyVJq1evlsfj0fr16zV16tRgTwMAAFjmvN9T09DQcNKfkiIjI1VRUaHGxkan7ciRI7rqqquUlJSkW2+9Vbt373b69u7dK6/Xq/T0dKfN7XYrNTVVZWVlp3xfv98fcAAAAHud91CTkZGh5cuXa8eOHTLGqLKyUsuXL1djY6O+/vprSVLfvn21YsUKvfbaa/rjH/+o5uZmjRw5Ul988YUkyev1SpI8Hk/A3B6Px+n7sYKCArndbudISko6j2cJAADa23kPNXPmzNHEiRM1YsQIhYaG6tZbb9XMmTNPvHnIibdPS0vTjBkzNGTIEI0ePVqvvvqqunfvrn//939v9fvm5+fL5/M5x+eff35OzgcAAFycznuoiYyM1IoVK3T06FHt27dPNTU1Sk5OVlRUlLp3797imNDQUA0dOlR79uyRJOdem7q6uoC6urq6U96HEx4erujo6IADAADY64I9pyY0NFQ9evRQp06dVFhYqJtvvtm5UvNjTU1Nev/995WQkCBJSklJUXx8vEpKSpwav9+v8vJypaWlXZD1AwCAi1vQ3346cuSIcwVFOnET765duxQbG6uePXsqPz9f+/fvd55F8/HHH6uiokKpqan69ttvtXjxYlVVVenFF1905njyySc1YsQI9e7dW4cOHdLChQv12Wef6Ze//KWkE9+Mys3N1VNPPaU+ffooJSVFc+bMUWJiojIzM9u4BQAAwAZBh5rKykqNHTvWeZ2XlydJmjlzplatWqXa2lrV1NQ4/U1NTVq0aJGqq6sVGhqqsWPHauvWrUpOTnZqvv32W91zzz3yer3q2rWrhg0bpq1bt+raa691ah566CHV19fr3nvv1aFDh/TTn/5Ub7755knfrAIAAJcmlzHGtPciLgS/3y+32y2fz8f9NQAAdBDBfH7zv58AAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYIOtRs3rxZkydPVmJiolwul9avX3/GMUuWLFH//v0VGRmpvn37avXq1QH9f/jDH/Szn/1MXbt2VdeuXZWenq6KioqAmrvuuksulyvgmDBhQrDLBwAAlgo61NTX12vw4MFasmTJWdUvXbpU+fn5euKJJ7R7927NmzdP2dnZev31152aTZs2adq0aXr77bdVVlampKQkjR8/Xvv37w+Ya8KECaqtrXWOl19+OdjlAwAAS7mMMabVg10urVu3TpmZmaesGTlypEaNGqWFCxc6bbNnz1Z5ebm2bNnS4pimpiZ17dpVzz33nGbMmCHpxJWaQ4cOndWVoZb4/X653W75fD5FR0e3ag4AAHBhBfP5fd7vqWloaFBERERAW2RkpCoqKtTY2NjimKNHj6qxsVGxsbEB7Zs2bVJcXJz69u2r+++/XwcPHjzt+/r9/oADAADY67yHmoyMDC1fvlw7duyQMUaVlZVavny5Ghsb9fXXX7c45uGHH1ZiYqLS09OdtgkTJmj16tUqKSnR008/rdLSUk2cOFFNTU0tzlFQUCC32+0cSUlJ5+X8AADAxaHz+X6DOXPmyOv1asSIETLGyOPxaObMmVqwYIFCQk7OVPPnz1dhYaE2bdoUcIVn6tSpzs8DBw7UoEGDdPXVV2vTpk0aN27cSfPk5+crLy/Pee33+wk2AABY7LxfqYmMjNSKFSt09OhR7du3TzU1NUpOTlZUVJS6d+8eUPv73/9e8+fP11tvvaVBgwaddt5evXqpW7du2rNnT4v94eHhio6ODjgAAIC9zvuVmh+EhoaqR48ekqTCwkLdfPPNAVdqFixYoH/+53/Wf/3Xf+n6668/43xffPGFDh48qISEhPO2ZgAA0HEEHWqOHDkScHVk79692rVrl2JjY9WzZ0/l5+dr//79zrNoPv74Y1VUVCg1NVXffvutFi9erKqqKr344ovOHE8//bTmzp2rNWvWKDk5WV6vV5LUpUsXdenSRUeOHNG8efN0++23Kz4+Xp9++qkeeugh9e7dWxkZGW3dAwAAYIGg//xUWVmpoUOHaujQoZKkvLw8DR06VHPnzpUk1dbWqqamxqlvamrSokWLNHjwYP3d3/2djh07pq1btyo5OdmpWbp0qb7//nvdcccdSkhIcI7f//73kqROnTrpvffe0y233KJrrrlGWVlZGjZsmN555x2Fh4e35fwBAIAl2vScmo6E59QAANDxXFTPqQEAALgQCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYIOtRs3rxZkydPVmJiolwul9avX3/GMUuWLFH//v0VGRmpvn37avXq1SfVrF27Vv369VNERIQGDhyoDRs2BPQbYzR37lwlJCQoMjJS6enp+uSTT4JdPgAAsFTQoaa+vl6DBw/WkiVLzqp+6dKlys/P1xNPPKHdu3dr3rx5ys7O1uuvv+7UbN26VdOmTVNWVpZ27typzMxMZWZmqqqqyqlZsGCBnn32WS1btkzl5eW6/PLLlZGRoWPHjgV7CgAAwEIuY4xp9WCXS+vWrVNmZuYpa0aOHKlRo0Zp4cKFTtvs2bNVXl6uLVu2SJKmTJmi+vp6FRUVOTUjRozQkCFDtGzZMhljlJiYqNmzZ+u3v/2tJMnn88nj8WjVqlWaOnXqGdfq9/vldrvl8/kUHR3dyjMGAAAXUjCf3+f9npqGhgZFREQEtEVGRqqiokKNjY2SpLKyMqWnpwfUZGRkqKysTJK0d+9eeb3egBq3263U1FSnpqX39fv9AQcAALDXeQ81GRkZWr58uXbs2CFjjCorK7V8+XI1Njbq66+/liR5vV55PJ6AcR6PR16v1+n/oe1UNT9WUFAgt9vtHElJSef61AAAwEXkvIeaOXPmaOLEiRoxYoRCQ0N16623aubMmSfePOT8vX1+fr58Pp9zfP755+ftvQAAQPs776EmMjJSK1as0NGjR7Vv3z7V1NQoOTlZUVFR6t69uyQpPj5edXV1AePq6uoUHx/v9P/QdqqaHwsPD1d0dHTAAQAA7HXBnlMTGhqqHj16qFOnTiosLNTNN9/sXKlJS0tTSUlJQH1xcbHS0tIkSSkpKYqPjw+o8fv9Ki8vd2oAAMClrXOwA44cOaI9e/Y4r/fu3atdu3YpNjZWPXv2VH5+vvbv3+88i+bjjz9WRUWFUlNT9e2332rx4sWqqqrSiy++6Mwxa9YsjR49WosWLdKkSZNUWFioyspKPf/885JOfMsqNzdXTz31lPr06aOUlBTNmTNHiYmJp/3mFQAAuHQEHWoqKys1duxY53VeXp4kaebMmVq1apVqa2tVU1Pj9Dc1NWnRokWqrq5WaGioxo4dq61btyo5OdmpGTlypNasWaPHHntMjz76qPr06aP169drwIABTs1DDz2k+vp63XvvvTp06JB++tOf6s033zzpm1UAAODS1Kbn1HQkPKcGAICO56J6Tg0AAMCFQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKwQ9L9J6Kh+eHCy3+9v55UAAICz9cPn9tn8A4RLJtQcPnxYkpSUlNTOKwEAAME6fPiw3G73aWsumf/91NzcrC+//FJRUVFyuVzndG6/36+kpCR9/vnn/F+pVmIP24b9azv2sO3Yw7Zh/1pmjNHhw4eVmJiokJDT3zVzyVypCQkJUY8ePc7re0RHR/OL2EbsYduwf23HHrYde9g27N/JznSF5gfcKAwAAKxAqAEAAFYg1JwD4eHhevzxxxUeHt7eS+mw2MO2Yf/ajj1sO/awbdi/trtkbhQGAAB240oNAACwAqEGAABYgVADAACsQKgBAABWINS00ZIlS5ScnKyIiAilpqaqoqKivZd00di8ebMmT56sxMREuVwurV+/PqDfGKO5c+cqISFBkZGRSk9P1yeffBJQ880332j69OmKjo5WTEyMsrKydOTIkQt4Fu2noKBAw4cPV1RUlOLi4pSZmanq6uqAmmPHjik7O1tXXHGFunTpottvv111dXUBNTU1NZo0aZIuu+wyxcXF6Xe/+52OHz9+IU+l3SxdulSDBg1yHmaWlpamv/zlL04/+xec+fPny+VyKTc312ljD0/viSeekMvlCjj69evn9LN/55hBqxUWFpqwsDCzYsUKs3v3bnPPPfeYmJgYU1dX195Luyhs2LDB/L//9//Mq6++aiSZdevWBfTPnz/fuN1us379evPf//3f5pZbbjEpKSnmu+++c2omTJhgBg8ebLZt22beeecd07t3bzNt2rQLfCbtIyMjw6xcudJUVVWZXbt2mZtuusn07NnTHDlyxKn51a9+ZZKSkkxJSYmprKw0I0aMMCNHjnT6jx8/bgYMGGDS09PNzp07zYYNG0y3bt1Mfn5+e5zSBfef//mf5o033jAff/yxqa6uNo8++qgJDQ01VVVVxhj2LxgVFRUmOTnZDBo0yMyaNctpZw9P7/HHHzc/+clPTG1trXN89dVXTj/7d24RatrghhtuMNnZ2c7rpqYmk5iYaAoKCtpxVRenH4ea5uZmEx8fbxYuXOi0HTp0yISHh5uXX37ZGGPMBx98YCSZ7du3OzV/+ctfjMvlMvv3779ga79YHDhwwEgypaWlxpgT+xUaGmrWrl3r1Hz44YdGkikrKzPGnAiWISEhxuv1OjVLly410dHRpqGh4cKewEWia9euZvny5exfEA4fPmz69OljiouLzejRo51Qwx6e2eOPP24GDx7cYh/7d+7x56dW+v7777Vjxw6lp6c7bSEhIUpPT1dZWVk7rqxj2Lt3r7xeb8D+ud1upaamOvtXVlammJgYXX/99U5Nenq6QkJCVF5efsHX3N58Pp8kKTY2VpK0Y8cONTY2Buxhv3791LNnz4A9HDhwoDwej1OTkZEhv9+v3bt3X8DVt7+mpiYVFhaqvr5eaWlp7F8QsrOzNWnSpIC9kvgdPFuffPKJEhMT1atXL02fPl01NTWS2L/z4ZL5h5bn2tdff62mpqaAXzRJ8ng8+uijj9ppVR2H1+uVpBb374c+r9eruLi4gP7OnTsrNjbWqblUNDc3Kzc3V6NGjdKAAQMkndifsLAwxcTEBNT+eA9b2uMf+i4F77//vtLS0nTs2DF16dJF69at07XXXqtdu3axf2ehsLBQ7777rrZv335SH7+DZ5aamqpVq1apb9++qq2t1bx58/Szn/1MVVVV7N95QKgBOoDs7GxVVVVpy5Yt7b2UDqdv377atWuXfD6f/vznP2vmzJkqLS1t72V1CJ9//rlmzZql4uJiRUREtPdyOqSJEyc6Pw8aNEipqam66qqr9Kc//UmRkZHtuDI78eenVurWrZs6dep00l3qdXV1io+Pb6dVdRw/7NHp9i8+Pl4HDhwI6D9+/Li++eabS2qPc3JyVFRUpLfffls9evRw2uPj4/X999/r0KFDAfU/3sOW9viHvktBWFiYevfurWHDhqmgoECDBw/Wv/zLv7B/Z2HHjh06cOCArrvuOnXu3FmdO3dWaWmpnn32WXXu3Fkej4c9DFJMTIyuueYa7dmzh9/B84BQ00phYWEaNmyYSkpKnLbm5maVlJQoLS2tHVfWMaSkpCg+Pj5g//x+v8rLy539S0tL06FDh7Rjxw6nZuPGjWpublZqauoFX/OFZoxRTk6O1q1bp40bNyolJSWgf9iwYQoNDQ3Yw+rqatXU1ATs4fvvvx8QDouLixUdHa1rr732wpzIRaa5uVkNDQ3s31kYN26c3n//fe3atcs5rr/+ek2fPt35mT0MzpEjR/Tpp58qISGB38Hzob3vVO7ICgsLTXh4uFm1apX54IMPzL333mtiYmIC7lK/lB0+fNjs3LnT7Ny500gyixcvNjt37jSfffaZMebEV7pjYmLMa6+9Zt577z1z6623tviV7qFDh5ry8nKzZcsW06dPn0vmK93333+/cbvdZtOmTQFfBz169KhT86tf/cr07NnTbNy40VRWVpq0tDSTlpbm9P/wddDx48ebXbt2mTfffNN07979kvk66COPPGJKS0vN3r17zXvvvWceeeQR43K5zFtvvWWMYf9a4/9++8kY9vBMZs+ebTZt2mT27t1r/va3v5n09HTTrVs3c+DAAWMM+3euEWra6F//9V9Nz549TVhYmLnhhhvMtm3b2ntJF423337bSDrpmDlzpjHmxNe658yZYzwejwkPDzfjxo0z1dXVAXMcPHjQTJs2zXTp0sVER0ebu+++2xw+fLgdzubCa2nvJJmVK1c6Nd9995359a9/bbp27Wouu+wy8/Of/9zU1tYGzLNv3z4zceJEExkZabp162Zmz55tGhsbL/DZtI9f/OIX5qqrrjJhYWGme/fuZty4cU6gMYb9a40fhxr28PSmTJliEhISTFhYmLnyyivNlClTzJ49e5x+9u/cchljTPtcIwIAADh3uKcGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYg1AAAACv8f4oeUKsg3wGmAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pn_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef_np(y_pred, y_true):\n",
    "\n",
    "    intersection = np.sum(y_true.flatten() * y_pred.flatten())\n",
    "    return (2. * intersection + smooth) / (np.sum(y_true).flatten() +\n",
    "                                           np.sum(y_pred).flatten() + smooth)\n",
    "\n",
    "\n",
    "def dice_loss(y_pred, y_true):\n",
    "\n",
    "    return 1.0 - dice_coef(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou_and_dice_np(pred, gt) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Compute IoU and Dice metrics for binary segmentation masks using PyTorch.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): Predicted binary mask (0 or 1), shape (H, W).\n",
    "        gt (torch.Tensor): Ground truth binary mask (0 or 1), shape (H, W).\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, float]: IoU and Dice scores.\n",
    "    \"\"\"\n",
    "    # Ensure binary masks (threshold at 0.5 for soft predictions)\n",
    "    pred = (pred > 0.5).astype(float)\n",
    "    gt = (gt > 0.5).astype(float)\n",
    "\n",
    "    # Compute intersection and union\n",
    "    intersection = np.sum(pred * gt)\n",
    "    union = np.sum(pred) + np.sum(gt) - intersection\n",
    "\n",
    "    # Compute IoU\n",
    "    iou = (intersection / union).item() if union > 0 else 0.0\n",
    "\n",
    "    # Compute Dice coefficient\n",
    "    dice = (2 * intersection /\n",
    "            (np.sum(pred) + np.sum(gt))).item() if (np.sum(pred) +\n",
    "                                                    np.sum(gt)) > 0 else 0.0\n",
    "\n",
    "    return iou, dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 44 is out of bounds for axis 0 with size 40",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m minimize\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load mask\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43mmasks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m44\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m      8\u001b[0m _, binary \u001b[38;5;241m=\u001b[39m _, mask\n\u001b[1;32m     10\u001b[0m H, W \u001b[38;5;241m=\u001b[39m binary\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mIndexError\u001b[0m: index 44 is out of bounds for axis 0 with size 40"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Load mask\n",
    "c = masks[44][0].astype(np.uint8)\n",
    "_, binary = _, mask\n",
    "\n",
    "H, W = binary.shape\n",
    "\n",
    "\n",
    "# Helper to draw a single circle mask\n",
    "def draw_circle(h, w, x, y, r):\n",
    "    Y, X = np.ogrid[:h, :w]\n",
    "    return ((X - x)**2 + (Y - y)**2) <= r**2\n",
    "\n",
    "\n",
    "# Loss function: overlap mismatch between union of circles and the binary mask\n",
    "def loss_fn(params):\n",
    "    x1, y1, r1, x2, y2, r2 = params\n",
    "    circle1 = draw_circle(H, W, x1, y1, r1)\n",
    "    circle2 = draw_circle(H, W, x2, y2, r2)\n",
    "    union = np.logical_or(circle1, circle2).astype(np.uint8)\n",
    "    iou, dice = compute_iou_and_dice_np(union, binary)\n",
    "    return -iou  #dice_coef_np(union, binary)\n",
    "\n",
    "\n",
    "# Initial guess: center of image and equal radii\n",
    "initial_guess = [W // 3, H // 2, 100, 2 * W // 3, H // 2, 1]\n",
    "\n",
    "# Bounds to constrain solution\n",
    "bounds = [\n",
    "    (0, W),\n",
    "    (0, H),\n",
    "    (5, W // 2),\n",
    "    (0, W),\n",
    "    (0, H),\n",
    "    (5, W // 2),\n",
    "]\n",
    "\n",
    "# Run optimization\n",
    "result = minimize(loss_fn, initial_guess, bounds=bounds, method='L-BFGS-B')\n",
    "x1, y1, r1, x2, y2, r2 = result.x\n",
    "\n",
    "circle1 = draw_circle(H, W, x1, y1, r1)\n",
    "circle2 = draw_circle(H, W, x2, y2, r2)\n",
    "union = np.logical_or(circle1, circle2).astype(np.uint8)\n",
    "# Visualize\n",
    "result_mask = np.zeros((H, W), dtype=np.uint8)\n",
    "cv2.circle(result_mask, (int(x1), int(y1)), int(r1), 1, -1)\n",
    "cv2.circle(result_mask, (int(x2), int(y2)), int(r2), 1, -1)\n",
    "result_mask *= 255\n",
    "\n",
    "overlay = cv2.merge([binary * 255, result_mask, np.zeros_like(result_mask)])\n",
    "\n",
    "plt.imshow(overlay)\n",
    "plt.title(\"Green: Fitted Circles | Red: Original Mask\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgb2gray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb2gray(isolated_pns[44])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.draw import ellipse\n",
    "from skimage.measure import label, regionprops, regionprops_table\n",
    "from skimage.transform import rotate\n",
    "\n",
    "image = upscaled_masks[44]\n",
    "\n",
    "label_img = label(image)\n",
    "regions = regionprops(label_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image, cmap=plt.cm.gray)\n",
    "\n",
    "for props in regions:\n",
    "    y0, x0 = props.centroid\n",
    "    orientation = props.orientation\n",
    "    x1 = x0 + math.cos(orientation) * 0.5 * props.axis_minor_length\n",
    "    y1 = y0 - math.sin(orientation) * 0.5 * props.axis_minor_length\n",
    "    x2 = x0 - math.sin(orientation) * 0.5 * props.axis_major_length\n",
    "    y2 = y0 - math.cos(orientation) * 0.5 * props.axis_major_length\n",
    "\n",
    "    ax.plot((x0, x1), (y0, y1), '-r', linewidth=1.5)\n",
    "    ax.plot((x0, x2), (y0, y2), '-r', linewidth=1.5)\n",
    "    ax.plot(x0, y0, '.g', markersize=15)\n",
    "\n",
    "    minr, minc, maxr, maxc = props.bbox\n",
    "    bx = (minc, maxc, maxc, minc, minc)\n",
    "    by = (minr, minr, maxr, maxr, minr)\n",
    "    # ax.plot(bx, by, '-b', linewidth=2.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[205], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m props \u001b[38;5;241m=\u001b[39m \u001b[43mregionprops_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_img\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ntua/phd/cellforge/cellforge_venv/lib/python3.12/site-packages/skimage/measure/_regionprops.py:1106\u001b[0m, in \u001b[0;36mregionprops_table\u001b[0;34m(label_image, intensity_image, properties, cache, separator, extra_properties, spacing)\u001b[0m\n\u001b[1;32m   1103\u001b[0m     out_d \u001b[38;5;241m=\u001b[39m _props_to_dict(regions, properties\u001b[38;5;241m=\u001b[39mproperties, separator\u001b[38;5;241m=\u001b[39mseparator)\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: v[:\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m out_d\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m-> 1106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_props_to_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproperties\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseparator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ntua/phd/cellforge/cellforge_venv/lib/python3.12/site-packages/skimage/measure/_regionprops.py:884\u001b[0m, in \u001b[0;36m_props_to_dict\u001b[0;34m(regions, properties, separator)\u001b[0m\n\u001b[1;32m    882\u001b[0m out \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    883\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(regions)\n\u001b[0;32m--> 884\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mregions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Copy the original property name so the output will have the\u001b[39;49;00m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# user-provided property name in the case of deprecated names.\u001b[39;49;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "props = regionprops_table(label_img, properties=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'centroid-0': array([263.81139122]),\n",
       " 'centroid-1': array([249.22875817]),\n",
       " 'orientation': array([-1.21167299]),\n",
       " 'axis_major_length': array([50.30582695]),\n",
       " 'axis_minor_length': array([27.92539493])}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "props\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pred_mask = mask[0]\n",
    "sample_gt_image = image_ar[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (224,) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Prediction mask thresholded\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43maxes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgray\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m axes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Mask > 0.4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m axes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/ntua/phd/cellforge/cellforge_venv/lib/python3.12/site-packages/matplotlib/__init__.py:1521\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1521\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1526\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1527\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1528\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/ntua/phd/cellforge/cellforge_venv/lib/python3.12/site-packages/matplotlib/axes/_axes.py:5945\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5943\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m-> 5945\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5946\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5948\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "File \u001b[0;32m~/ntua/phd/cellforge/cellforge_venv/lib/python3.12/site-packages/matplotlib/image.py:675\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m    674\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/ntua/phd/cellforge/cellforge_venv/lib/python3.12/site-packages/matplotlib/image.py:643\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    641\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[1;32m    649\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (224,) for image data"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'source_image', 'sample_gt_mask', and 'sample_pred_mask' are numpy arrays\n",
    "# Display side by side\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Original ground truth mask\n",
    "axes[0].imshow(sample_gt_mask.numpy(), cmap='gray')\n",
    "axes[0].set_title(\"Ground Truth Mask\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Prediction mask thresholded\n",
    "axes[1].imshow(mask[0] > 0.4, cmap='gray')\n",
    "axes[1].set_title(\"Predicted Mask > 0.4\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Overlay source image with ground truth and predicted masks\n",
    "overlay_image = all_images[0][3][0].numpy().copy()\n",
    "# Adjust overlay to include masks for better visibility\n",
    "\n",
    "axes[2].imshow(overlay_image, cmap='gray')\n",
    "axes[2].set_title(\"Overlay with Source Image\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellforge_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
