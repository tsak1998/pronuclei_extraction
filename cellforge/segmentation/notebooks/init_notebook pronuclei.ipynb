{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "# from unet import AttUNet, UNet, UNetWithPretrainedEncoder\n",
    "# from dataloader import ImageDataset, TransformWrapper\n",
    "import os\n",
    "import random\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torchvision import transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from PIL.ImageFile import ImageFile\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def apply_clahe(pil_img):\n",
    "    # Convert PIL image to NumPy array\n",
    "    img = np.array(pil_img)\n",
    "\n",
    "    # If grayscale\n",
    "    if len(img.shape) == 2:\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        img_clahe = clahe.apply(img)\n",
    "    # If RGB\n",
    "    elif len(img.shape) == 3:\n",
    "        img_lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "        l, a, b = cv2.split(img_lab)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        l_clahe = clahe.apply(l)\n",
    "        img_lab_clahe = cv2.merge((l_clahe, a, b))\n",
    "        img_clahe = cv2.cvtColor(img_lab_clahe, cv2.COLOR_LAB2RGB)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported image format\")\n",
    "\n",
    "    # Convert back to PIL image\n",
    "    return Image.fromarray(img_clahe)\n",
    "\n",
    "\n",
    "class TransformWrapper:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.image_transforms = T.Compose([\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.RandomVerticalFlip(p=0.5),\n",
    "            T.RandomRotation(degrees=[30, 60, 90, 120, 150]),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        self.mask_transforms = T.Compose([\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.RandomVerticalFlip(p=0.5),\n",
    "            T.RandomRotation(degrees=45),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        # Apply the same random seed to ensure consistent transformations\n",
    "        seed = torch.randint(0, 2**32, (1, )).item()\n",
    "        torch.manual_seed(seed)\n",
    "        image = self.image_transforms(image)\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "        mask = self.mask_transforms(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "class ImageDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 images: list[ImageFile],\n",
    "                 masks: list[ImageFile],\n",
    "                 transform: bool = False,\n",
    "                 image_size: int = 224):\n",
    "\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self,\n",
    "                    index) -> tuple[torch.FloatTensor, torch.FloatTensor]:\n",
    "        image = self.images[index]\n",
    "        mask = self.masks[index]\n",
    "\n",
    "        image = image.resize((self.image_size, self.image_size),\n",
    "                             Image.Resampling.LANCZOS)\n",
    "        mask = mask.resize((self.image_size, self.image_size),\n",
    "                           Image.Resampling.LANCZOS)\n",
    "\n",
    "        image = apply_clahe(image)\n",
    "        image = T.GaussianBlur(3)(image)\n",
    "\n",
    "        mask = mask.convert(\"L\")  # Ensure mask is in grayscale\n",
    "        binary_threshold = 100  # Adjust this threshold as needed\n",
    "        mask = mask.point(lambda p: 255 if p > binary_threshold else 0)\n",
    "        mask = mask.convert('1')\n",
    "        \n",
    "\n",
    "        normalize_tensor = T.Compose([\n",
    "            T.Lambda(lambda x: x.convert(\"RGB\")),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            T.Lambda(lambda x: x),\n",
    "        ])\n",
    "\n",
    "        if self.transform:\n",
    "            angle = random.uniform(-90, 90)\n",
    "            \n",
    "            image = F.rotate(image, angle)\n",
    "            image = normalize_tensor(image)\n",
    "            mask = T.ToTensor()(F.rotate(mask, angle))\n",
    "            return image, mask\n",
    "\n",
    "        return normalize_tensor(image), T.ToTensor()(mask)\n",
    "\n",
    "data_pth = Path(\n",
    "    '/Users/tsakalis/Downloads/ECImageAnalysisMouse/New Binary Masks')\n",
    "blastocyst_pth = Path('/home/tsakalis/ntua/phd/cellforge/cellforge/data')\n",
    "\n",
    "full_path = \"/Users/tsakalis/ntua/cellforge/data/D2016.07.08_S1366_I149_11\"\n",
    "\n",
    "blastocyst_images_pth = blastocyst_pth / 'annotation_pn/images_pn'\n",
    "blastocyst_msk_pth = blastocyst_pth / 'annotation_pn/masks_pn'\n",
    "\n",
    "smooth = 1e-15\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "\n",
    "def dice_coef(y_pred, y_true):\n",
    "\n",
    "    intersection = torch.sum(y_true.flatten() * y_pred.flatten())\n",
    "    return (2. * intersection + smooth) / (\n",
    "        torch.sum(y_true).flatten() + torch.sum(y_pred).flatten() + smooth)\n",
    "\n",
    "\n",
    "def dice_loss(y_pred, y_true):\n",
    "\n",
    "    return 1.0 - dice_coef(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_file_paths = sorted(list((data_pth / \"images\").glob('*.png')),\n",
    "#                           key=lambda x: x.stem)\n",
    "# mask_file_paths = sorted(list((data_pth / \"masks\").glob('*.png')),\n",
    "#                          key=lambda x: x.stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.ndimage import center_of_mass\n",
    "\n",
    "\n",
    "def crop_around_center(image: Image.Image, mask: Image.Image,\n",
    "                       crop_size: int) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Finds the center of mass of the non-zero pixels in the image\n",
    "    and crops the image around that point.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image.Image): Input image (grayscale or binary recommended).\n",
    "        crop_size (int): Size of the square crop (e.g., 128 for 128x128 crop).\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image.Image: Cropped image around the center of mass.\n",
    "    \"\"\"\n",
    "    # Convert image to grayscale and NumPy array\n",
    "    image_array = np.array(image.convert(\"L\"))\n",
    "\n",
    "    # Find the center of mass of non-zero pixels\n",
    "    com = center_of_mass(image_array)\n",
    "\n",
    "    # Round to integers for pixel indexing\n",
    "    center_y, center_x = map(int, com)\n",
    "\n",
    "    # Calculate crop box\n",
    "    half_crop = crop_size // 2\n",
    "    left = max(center_x - half_crop, 0)\n",
    "    upper = max(center_y - half_crop, 0)\n",
    "    right = min(center_x + half_crop, image.width)\n",
    "    lower = min(center_y + half_crop, image.height)\n",
    "\n",
    "    # Crop the image\n",
    "    cropped_image = image.crop((left, upper, right, lower))\n",
    "\n",
    "    cropped_mask = mask.crop((left, upper, right, lower))\n",
    "    return cropped_image, cropped_mask\n",
    "\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file_paths = sorted(list(blastocyst_images_pth.glob('*.jpg')),\n",
    "                          key=lambda x: x.stem)\n",
    "mask_file_paths = sorted(list(blastocyst_msk_pth.glob('*.png')),\n",
    "                         key=lambda x: x.stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/tsakalis/ntua/phd/cellforge/cellforge/data/annotation_pn/images_pn')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blastocyst_images_pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 610/610 [00:00<00:00, 10105.20it/s]\n",
      "100%|██████████| 610/610 [00:00<00:00, 16996.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# image_file_paths = sorted(list((data_pth / \"images\").glob('*.jpg')),\n",
    "#                           key=lambda x: x.stem)\n",
    "# mask_file_paths = sorted(list((data_pth / \"masks\").glob('*.png')),\n",
    "#                          key=lambda x: x.stem)\n",
    "\n",
    "# print(\"... Loading images ...\")\n",
    "images = [Image.open(img_path) for img_path in tqdm(image_file_paths)]\n",
    "masks = [Image.open(msk_pth) for msk_pth in tqdm(mask_file_paths)]\n",
    "\n",
    "c = list(zip(images, masks))\n",
    "import random\n",
    "\n",
    "random.shuffle(c)\n",
    "\n",
    "images, masks = zip(*c)\n",
    "\n",
    "# def remove_alpha(img):\n",
    "#     if img.mode == 'RGBA':  # If image has an alpha channel\n",
    "#         background = Image.new('RGB', img.size,\n",
    "#                                (255, 255, 255))  # Create white background\n",
    "#         background.paste(img, mask=img.split()[3])  # Use alpha channel as mask\n",
    "#         return background\n",
    "#     return img  # Return unchanged if no alpha channel\n",
    "\n",
    "# # Process images and masks\n",
    "# images = [remove_alpha(img) for img in tqdm(images)]\n",
    "# masks = [remove_alpha(msk) for msk in tqdm(masks)]\n",
    "\n",
    "# cropped_images = []\n",
    "# cropped_masks = []\n",
    "# for image, mask in zip(images, masks):\n",
    "\n",
    "#     cropped, cropped_mask = crop_around_center(image, mask, crop_size=200 * 3)\n",
    "\n",
    "#     cropped_images.append(cropped)\n",
    "#     cropped_masks.append(cropped_mask)\n",
    "\n",
    "train_dataset = ImageDataset(images=images[:400],\n",
    "                             masks=masks[:400],\n",
    "                             transform=True)\n",
    "val_dataset = ImageDataset(images=images[400:800], masks=masks[400:800])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision import transforms as T\n",
    "import torchvision.transforms.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for im, gt in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsakalis/ntua/phd/cellforge/cellforge_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=\n",
    "    \"resnext101_32x48d\",  # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\n",
    "    \"instagram\",  # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=\n",
    "    3,  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,  # model output channels (number of classes in your dataset)\n",
    ")\n",
    "model.to(device)\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def validate(model, val_dataloader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img_batch, gt_msk_batch in val_dataloader:\n",
    "\n",
    "            img_batch = img_batch.to(device)\n",
    "            gt_msk_batch = gt_msk_batch.to(device)\n",
    "\n",
    "            pred_mask = model(img_batch)\n",
    "\n",
    "            loss = dice_loss(torch.sigmoid(pred_mask), gt_msk_batch)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    mean_val_loss = val_loss / len(val_dataloader)\n",
    "    return mean_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1417433/3522360610.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]/tmp/ipykernel_1417433/2306933468.py:8: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "0.9592396020889282: 100%|██████████| 25/25 [00:08<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | TrainLoss: 0.9660129523277283 ValLoss: 0.9641322940587997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nan: 100%|██████████| 25/25 [00:06<00:00,  3.65it/s]              \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m     progress_bar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;28mstr\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem()))\n\u001b[1;32m     20\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 22\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | TrainLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ValLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     26\u001b[0m )\n",
      "Cell \u001b[0;32mIn[12], line 18\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(model, val_dataloader)\u001b[0m\n\u001b[1;32m     14\u001b[0m         pred_mask \u001b[38;5;241m=\u001b[39m model(img_batch)\n\u001b[1;32m     16\u001b[0m         loss \u001b[38;5;241m=\u001b[39m dice_loss(torch\u001b[38;5;241m.\u001b[39msigmoid(pred_mask), gt_msk_batch)\n\u001b[0;32m---> 18\u001b[0m         val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m mean_val_loss \u001b[38;5;241m=\u001b[39m val_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(val_dataloader)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mean_val_loss\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    progress_bar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "\n",
    "    train_loss = 0\n",
    "    for img_batch, gt_msk_batch in progress_bar:\n",
    "        img_batch = img_batch.to(device)\n",
    "        gt_msk_batch = gt_msk_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            pred_mask = model()\n",
    "\n",
    "            loss = dice_loss(torch.sigmoid(pred_mask), gt_msk_batch.to(device))\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        progress_bar.set_description(str(loss.item()))\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    val_loss = validate(model, val_dataloader)\n",
    "\n",
    "    print(\n",
    "        f'Epoch {epoch+1} | TrainLoss: {train_loss/len(train_dataloader)} ValLoss: {val_loss}'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mimg_batch\u001b[49m[\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m0\u001b[39m,:]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img_batch' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img_batch[10,0,:].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU: 0.8643, Dice: 0.9272\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def compute_iou_and_dice(pred: torch.Tensor,\n",
    "                         gt: torch.Tensor) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Compute IoU and Dice metrics for binary segmentation masks using PyTorch.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): Predicted binary mask (0 or 1), shape (H, W).\n",
    "        gt (torch.Tensor): Ground truth binary mask (0 or 1), shape (H, W).\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, float]: IoU and Dice scores.\n",
    "    \"\"\"\n",
    "    # Ensure binary masks (threshold at 0.5 for soft predictions)\n",
    "    pred = (pred > 0.5).float()\n",
    "    gt = (gt > 0.5).float()\n",
    "\n",
    "    # Compute intersection and union\n",
    "    intersection = torch.sum(pred * gt)\n",
    "    union = torch.sum(pred) + torch.sum(gt) - intersection\n",
    "\n",
    "    # Compute IoU\n",
    "    iou = (intersection / union).item() if union > 0 else 0.0\n",
    "\n",
    "    # Compute Dice coefficient\n",
    "    dice = (2 * intersection / (torch.sum(pred) + torch.sum(gt))).item() if (\n",
    "        torch.sum(pred) + torch.sum(gt)) > 0 else 0.0\n",
    "\n",
    "    return iou, dice\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# pred_mask = torch.randint(0, 2, (256, 256),\n",
    "#                           dtype=torch.float32)  # Example predicted mask\n",
    "# gt_mask = torch.randint(0, 2, (256, 256),\n",
    "#                         dtype=torch.float32)  # Example ground truth mask\n",
    "\n",
    "iou_fn, dice_fn = compute_iou_and_dice(torch.sigmoid(pred_mask),\n",
    "                                       gt_msk_batch.to(device))\n",
    "print(f\"IoU: {iou_fn:.4f}, Dice: {dice_fn:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/home/tsakalis/ntua/phd/cellforge/cellforge/model_weights/big_good_pn_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/tsakalis/ntua/phd/cellforge/cellforge/model_weights/good_pn_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/home/tsakalis/ntua/phd/cellforge/cellforge/model_weights/good_pn_model.pt\", weights_only=True))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "gt_masks_all = []\n",
    "pred_masks_all = []\n",
    "\n",
    "all_images = []\n",
    "\n",
    "\n",
    "for img_batch, gt_msk_batch in val_dataloader:\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_mask = model(img_batch.to(device))\n",
    "    all_images.append(img_batch)\n",
    "    pred_masks_all.append(pred_mask.to('cpu'))\n",
    "    gt_masks_all.append(gt_msk_batch.to('cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import JaccardIndex, Dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU: 0.837\n",
      "Dice Coeff: 0.911\n"
     ]
    }
   ],
   "source": [
    "iou_fn = JaccardIndex(task='binary', threshold=0.90)\n",
    "dice_fn = Dice(threshold=0.90)\n",
    "\n",
    "iou_fn(torch.sigmoid(torch.vstack(pred_masks_all)), torch.vstack(gt_masks_all))\n",
    "\n",
    "dice_fn(\n",
    "    torch.sigmoid(torch.vstack(pred_masks_all)),\n",
    "    torch.vstack(gt_masks_all).long(),\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"IoU: {iou_fn(torch.sigmoid(torch.vstack(pred_masks_all)), torch.vstack(gt_masks_all)):.3f}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Dice Coeff: {dice_fn(torch.sigmoid(torch.vstack(pred_masks_all)), torch.vstack(gt_masks_all).long()):.3f}\"\n",
    ")\n",
    "\n",
    "sample_pred_mask = torch.sigmoid(pred_masks_all[0][3][0].detach())\n",
    "\n",
    "sample_gt_mask = gt_masks_all[0][3][0].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 560/560 [00:00<00:00, 43937.49it/s]\n"
     ]
    }
   ],
   "source": [
    "sample_path = Path(\n",
    "    \"/home/tsakalis/ntua/phd/cellforge/cellforge/data/D2016.10.18_S1418_I149_6\")\n",
    "\n",
    "image_file_paths = sorted(list(sample_path.glob('*.jpg')),\n",
    "                          key=lambda x: int(x.stem))\n",
    "\n",
    "images = [Image.open(img_path) for img_path in tqdm(image_file_paths)][:170]\n",
    "\n",
    "# def remove_alpha(img):\n",
    "#     if img.mode == 'RGBA':  # If image has an alpha channel\n",
    "#         background = Image.new('RGB', img.size,\n",
    "#                                (255, 255, 255))  # Create white background\n",
    "#         background.paste(img, mask=img.split()[3])  # Use alpha channel as mask\n",
    "#         return background\n",
    "#     return img  # Return unchanged if no alpha channel\n",
    "\n",
    "# # Process images and masks\n",
    "# images = [remove_alpha(img) for img in tqdm(images)]\n",
    "# masks = [remove_alpha(msk) for msk in tqdm(masks)]\n",
    "\n",
    "# cropped_images = []\n",
    "# cropped_masks = []\n",
    "# for image, mask in zip(images, masks):\n",
    "\n",
    "#     cropped, cropped_mask = crop_around_center(image, mask, crop_size=200 * 3)\n",
    "\n",
    "#     cropped_images.append(cropped)\n",
    "#     cropped_masks.append(cropped_mask)\n",
    "\n",
    "val_dataset = ImageDataset(images=images, masks=images)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "527182"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for inpt_images, _ in val_dataloader: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_mask = model(inpt_images.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = (torch.sigmoid(pred_mask)>0.9).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tsakalis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsakalis/ntua/phd/cellforge/cellforge_venv/lib/python3.12/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/home/tsakalis/ntua/phd/cellforge/cellforge_venv/lib/python3.12/site-packages/IPython/core/magics/osm.py:428: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage.morphology import binary_dilation\n",
    "\n",
    "overlay_color = (255, 0, 0, 128)  # red with 50% transparency\n",
    "composited_images = []\n",
    "\n",
    "pn_size = []\n",
    "final_images = []\n",
    "upscaled_masks = []\n",
    "isolated_pns = []\n",
    "for pil_img, mask in zip(images[:], masks[:]):\n",
    "    # Ensure the mask is 2D by removing extra dimensions\n",
    "    # pil_img = pil_img.resize((224, 224), Image.Resampling.LANCZOS)\n",
    "    image_ar = np.stack(3*[np.array(pil_img)])\n",
    "    \n",
    "    upscaled_mask = cv2.resize(mask[0].astype(np.uint8), (500,500), interpolation=cv2.INTER_NEAREST)\n",
    "    pn_size.append(upscaled_mask.sum())\n",
    "\n",
    "    upscaled_masks.append(upscaled_mask)\n",
    "    image_pn_isolated = image_ar.copy()\n",
    "    image_pn_isolated[:,~binary_dilation(upscaled_mask).astype(bool)] = 0\n",
    "    isolated_pns.append(image_pn_isolated.transpose(1,2,0))\n",
    "    image_ar[0,upscaled_mask.astype(bool)] = 120\n",
    "\n",
    "\n",
    "    final_images.append(Image.fromarray(image_ar.transpose(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(isolated_pns[44],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Assume final_images is already populated with (224, 224, 3) numpy arrays ---\n",
    "\n",
    "# Frame size of input frames\n",
    "frame_height, frame_width = 500, 500\n",
    "\n",
    "# Create a dummy figure to measure the graph size\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([0, 1], [0, 1])\n",
    "fig.canvas.draw()\n",
    "graph_h, graph_w = fig.canvas.get_width_height()\n",
    "plt.close(fig)\n",
    "\n",
    "# Resize the graph to match the height of the frame\n",
    "scale = frame_height / graph_h\n",
    "graph_resized_width = int(graph_w * scale)\n",
    "\n",
    "# Final output size (frame + graph side by side)\n",
    "output_size = (frame_width + graph_resized_width, frame_height)\n",
    "\n",
    "# Setup output writer\n",
    "output = cv2.VideoWriter(\n",
    "    \"accumulated_pn_area_sample4_big.mp4\",\n",
    "    cv2.VideoWriter_fourcc(*'XVID'),\n",
    "    5,  # FPS\n",
    "    output_size\n",
    ")\n",
    "\n",
    "# Dummy video input just for structure (optional)\n",
    "cap = cv2.VideoCapture(\"input.mp4\")\n",
    "\n",
    "pn_size = []\n",
    "# Process frames\n",
    "for frame_idx, frame in enumerate(final_images):\n",
    "    # Generate plot data (example: sine wave)\n",
    "    x = np.linspace(0, 4 * np.pi, 200)\n",
    "    y = np.sin(x + frame_idx * 0.1)\n",
    "    pn_size.append(masks[frame_idx][0].sum())\n",
    "    x = np.arange(start=0, stop = frame_idx+1, step=1)\n",
    "    \n",
    "    # Create and render plot\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x,pn_size)\n",
    "    \n",
    "    # ax.set_ylim(-1.5, 1.5)\n",
    "    ax.set_title(f\"Accumulated Pronuclei Size (Frame {frame_idx})\")\n",
    "    ax.set_xlabel('Frame')\n",
    "    ax.set_ylabel('Accumulated Area')\n",
    "    fig.tight_layout()\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    # Extract RGB image from canvas (modern method)\n",
    "    plot_img = np.asarray(fig.canvas.buffer_rgba())[:, :, :3]  # Drop alpha\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Resize plot to match video height\n",
    "    plot_resized = cv2.resize(plot_img, (graph_resized_width, frame_height))\n",
    "    plot_bgr = cv2.cvtColor(plot_resized, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Combine original frame and graph\n",
    "    combined = np.hstack((np.array(frame), plot_bgr))\n",
    "\n",
    "    # Write and display\n",
    "    output.write(combined)\n",
    "    cv2.imshow(\"output\", combined)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cv2.destroyAllWindows()\n",
    "output.release()\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef_np(y_pred, y_true):\n",
    "\n",
    "    intersection = np.sum(y_true.flatten() * y_pred.flatten())\n",
    "    return (2. * intersection + smooth) / (\n",
    "        np.sum(y_true).flatten() + np.sum(y_pred).flatten() + smooth)\n",
    "\n",
    "\n",
    "def dice_loss(y_pred, y_true):\n",
    "\n",
    "    return 1.0 - dice_coef(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou_and_dice_np(pred, gt) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Compute IoU and Dice metrics for binary segmentation masks using PyTorch.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): Predicted binary mask (0 or 1), shape (H, W).\n",
    "        gt (torch.Tensor): Ground truth binary mask (0 or 1), shape (H, W).\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, float]: IoU and Dice scores.\n",
    "    \"\"\"\n",
    "    # Ensure binary masks (threshold at 0.5 for soft predictions)\n",
    "    pred = (pred > 0.5).astype(float)\n",
    "    gt = (gt > 0.5).astype(float)\n",
    "\n",
    "    # Compute intersection and union\n",
    "    intersection = np.sum(pred * gt)\n",
    "    union = np.sum(pred) + np.sum(gt) - intersection\n",
    "\n",
    "    # Compute IoU\n",
    "    iou = (intersection / union).item() if union > 0 else 0.0\n",
    "\n",
    "    # Compute Dice coefficient\n",
    "    dice = (2 * intersection / (np.sum(pred) + np.sum(gt))).item() if (\n",
    "        np.sum(pred) + np.sum(gt)) > 0 else 0.0\n",
    "\n",
    "    return iou, dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[183], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m c \u001b[38;5;241m=\u001b[39m masks[\u001b[38;5;241m44\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m      8\u001b[0m _, binary \u001b[38;5;241m=\u001b[39m _, mask\n\u001b[0;32m---> 10\u001b[0m H, W \u001b[38;5;241m=\u001b[39m binary\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Helper to draw a single circle mask\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_circle\u001b[39m(h, w, x, y, r):\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Load mask\n",
    "c = masks[44][0].astype(np.uint8)\n",
    "_, binary = _, mask\n",
    "\n",
    "H, W = binary.shape\n",
    "\n",
    "# Helper to draw a single circle mask\n",
    "def draw_circle(h, w, x, y, r):\n",
    "    Y, X = np.ogrid[:h, :w]\n",
    "    return ((X - x)**2 + (Y - y)**2) <= r**2\n",
    "\n",
    "# Loss function: overlap mismatch between union of circles and the binary mask\n",
    "def loss_fn(params):\n",
    "    x1, y1, r1, x2, y2, r2 = params\n",
    "    circle1 = draw_circle(H, W, x1, y1, r1)\n",
    "    circle2 = draw_circle(H, W, x2, y2, r2)\n",
    "    union = np.logical_or(circle1, circle2).astype(np.uint8)\n",
    "    iou, dice = compute_iou_and_dice_np(union,binary)\n",
    "    return -iou#dice_coef_np(union, binary)\n",
    "    \n",
    "\n",
    "# Initial guess: center of image and equal radii\n",
    "initial_guess = [W//3, H//2, 100, 2*W//3, H//2, 1]\n",
    "\n",
    "# Bounds to constrain solution\n",
    "bounds = [\n",
    "    (0, W), (0, H), (5, W//2),\n",
    "    (0, W), (0, H), (5, W//2),\n",
    "]\n",
    "\n",
    "# Run optimization\n",
    "result = minimize(loss_fn, initial_guess, bounds=bounds, method='L-BFGS-B')\n",
    "x1, y1, r1, x2, y2, r2 = result.x\n",
    "\n",
    "circle1 = draw_circle(H, W, x1, y1, r1)\n",
    "circle2 = draw_circle(H, W, x2, y2, r2)\n",
    "union = np.logical_or(circle1, circle2).astype(np.uint8)\n",
    "# Visualize\n",
    "result_mask = np.zeros((H, W), dtype=np.uint8)\n",
    "cv2.circle(result_mask, (int(x1), int(y1)), int(r1), 1, -1)\n",
    "cv2.circle(result_mask, (int(x2), int(y2)), int(r2), 1, -1)\n",
    "result_mask *= 255\n",
    "\n",
    "overlay = cv2.merge([binary * 255, result_mask, np.zeros_like(result_mask)])\n",
    "\n",
    "plt.imshow(overlay)\n",
    "plt.title(\"Green: Fitted Circles | Red: Original Mask\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgb2gray\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb2gray(isolated_pns[44])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.draw import ellipse\n",
    "from skimage.measure import label, regionprops, regionprops_table\n",
    "from skimage.transform import rotate\n",
    "\n",
    "\n",
    "image = upscaled_masks[44]\n",
    "\n",
    "label_img = label(image)\n",
    "regions = regionprops(label_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image, cmap=plt.cm.gray)\n",
    "\n",
    "for props in regions:\n",
    "    y0, x0 = props.centroid\n",
    "    orientation = props.orientation\n",
    "    x1 = x0 + math.cos(orientation) * 0.5 * props.axis_minor_length\n",
    "    y1 = y0 - math.sin(orientation) * 0.5 * props.axis_minor_length\n",
    "    x2 = x0 - math.sin(orientation) * 0.5 * props.axis_major_length\n",
    "    y2 = y0 - math.cos(orientation) * 0.5 * props.axis_major_length\n",
    "\n",
    "    ax.plot((x0, x1), (y0, y1), '-r', linewidth=1.5)\n",
    "    ax.plot((x0, x2), (y0, y2), '-r', linewidth=1.5)\n",
    "    ax.plot(x0, y0, '.g', markersize=15)\n",
    "\n",
    "    minr, minc, maxr, maxc = props.bbox\n",
    "    bx = (minc, maxc, maxc, minc, minc)\n",
    "    by = (minr, minr, maxr, maxr, minr)\n",
    "    # ax.plot(bx, by, '-b', linewidth=2.5)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[205], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m props \u001b[38;5;241m=\u001b[39m \u001b[43mregionprops_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_img\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ntua/phd/cellforge/cellforge_venv/lib/python3.12/site-packages/skimage/measure/_regionprops.py:1106\u001b[0m, in \u001b[0;36mregionprops_table\u001b[0;34m(label_image, intensity_image, properties, cache, separator, extra_properties, spacing)\u001b[0m\n\u001b[1;32m   1103\u001b[0m     out_d \u001b[38;5;241m=\u001b[39m _props_to_dict(regions, properties\u001b[38;5;241m=\u001b[39mproperties, separator\u001b[38;5;241m=\u001b[39mseparator)\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: v[:\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m out_d\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m-> 1106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_props_to_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproperties\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseparator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ntua/phd/cellforge/cellforge_venv/lib/python3.12/site-packages/skimage/measure/_regionprops.py:884\u001b[0m, in \u001b[0;36m_props_to_dict\u001b[0;34m(regions, properties, separator)\u001b[0m\n\u001b[1;32m    882\u001b[0m out \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    883\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(regions)\n\u001b[0;32m--> 884\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mregions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Copy the original property name so the output will have the\u001b[39;49;00m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# user-provided property name in the case of deprecated names.\u001b[39;49;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "props = regionprops_table(\n",
    "    label_img,\n",
    "    properties=None\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'centroid-0': array([263.81139122]),\n",
       " 'centroid-1': array([249.22875817]),\n",
       " 'orientation': array([-1.21167299]),\n",
       " 'axis_major_length': array([50.30582695]),\n",
       " 'axis_minor_length': array([27.92539493])}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "props\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pred_mask = mask[0]\n",
    "sample_gt_image = image_ar[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (224,) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Prediction mask thresholded\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43maxes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgray\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m axes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Mask > 0.4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m axes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/ntua/phd/cellforge/cellforge_venv/lib/python3.12/site-packages/matplotlib/__init__.py:1521\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1521\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1526\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1527\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1528\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/ntua/phd/cellforge/cellforge_venv/lib/python3.12/site-packages/matplotlib/axes/_axes.py:5945\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5943\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m-> 5945\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5946\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5948\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "File \u001b[0;32m~/ntua/phd/cellforge/cellforge_venv/lib/python3.12/site-packages/matplotlib/image.py:675\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m    674\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/ntua/phd/cellforge/cellforge_venv/lib/python3.12/site-packages/matplotlib/image.py:643\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    641\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[1;32m    649\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (224,) for image data"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'source_image', 'sample_gt_mask', and 'sample_pred_mask' are numpy arrays\n",
    "# Display side by side\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Original ground truth mask\n",
    "axes[0].imshow(sample_gt_mask.numpy(), cmap='gray')\n",
    "axes[0].set_title(\"Ground Truth Mask\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Prediction mask thresholded\n",
    "axes[1].imshow(mask[0] > 0.4, cmap='gray')\n",
    "axes[1].set_title(\"Predicted Mask > 0.4\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Overlay source image with ground truth and predicted masks\n",
    "overlay_image = all_images[0][3][0].numpy().copy()\n",
    "# Adjust overlay to include masks for better visibility\n",
    "\n",
    "axes[2].imshow(overlay_image, cmap='gray')\n",
    "axes[2].set_title(\"Overlay with Source Image\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellforge_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
